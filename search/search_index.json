{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ddb (for Docker Devbox) \u00b6 Erase environment differences, make developers happy ! ddb automates application configuration so differences between development, staging and production environment can be erased. It provides features to generate, activate and adjust configuration files based on a single overridable and extendable configuration , while enhancing the developer experience and reducing manual operations . Primarly designed for docker-compose and docker-devbox , this tool makes the developer forget about the docker hard stuff by providing commands right into it's PATH, so it's experience looks like everything is native and locally installed. Thanks to a pluggable, event based and easy to extend architecture, it can bring powerful configuration automation to any technical context . Install \u00b6 ddb should most often be installed by docker-devbox You should better install the whole docker-devbox toolkit to enjoy the experience. docker-devbox automatically installs ddb as a dependency, along some helper docker containers . Only advanced users should install ddb on their own. If you are not sure what to do, do not install ddb on your own, but follow docker-devbox installation docs. ddb is supported on Linux , Windows and MacOS . You can download binary releases on github , or install on Python 3.5+ with pip. pip install docker-devbox-ddb","title":"Home"},{"location":"#ddb-for-docker-devbox","text":"Erase environment differences, make developers happy ! ddb automates application configuration so differences between development, staging and production environment can be erased. It provides features to generate, activate and adjust configuration files based on a single overridable and extendable configuration , while enhancing the developer experience and reducing manual operations . Primarly designed for docker-compose and docker-devbox , this tool makes the developer forget about the docker hard stuff by providing commands right into it's PATH, so it's experience looks like everything is native and locally installed. Thanks to a pluggable, event based and easy to extend architecture, it can bring powerful configuration automation to any technical context .","title":"ddb (for Docker Devbox)"},{"location":"#install","text":"ddb should most often be installed by docker-devbox You should better install the whole docker-devbox toolkit to enjoy the experience. docker-devbox automatically installs ddb as a dependency, along some helper docker containers . Only advanced users should install ddb on their own. If you are not sure what to do, do not install ddb on your own, but follow docker-devbox installation docs. ddb is supported on Linux , Windows and MacOS . You can download binary releases on github , or install on Python 3.5+ with pip. pip install docker-devbox-ddb","title":"Install"},{"location":"commands/","text":"Commands \u00b6 If you run ddb with no argument, the usage is displayed. ddb usage usage: ddb [-h] [-v] [-vv] [-s] [-x] [-c] [-w] [-ff] [--version] {init,configure,features,config,info,self-update,run,activate,deactivate,check-activated} ... positional arguments: {init,configure,features,config,info,self-update,run,activate,deactivate,check-activated} Available commands init Initialize the environment configure Configure the environment features List enabled features config Display effective configuration info Display useful information self-update Update ddb to latest version run Display command to run project binary activate Write a shell script to be executed to activate environment deactivate Write a shell script to be executed to deactivate environment check-activated Check if project is activated in current shell optional arguments: -h, --help show this help message and exit -v, --verbose Enable more logs -vv, --very-verbose Enable even more logs -s, --silent Disable all logs -x, --exceptions Display exceptions on errors -c, --clear-cache Clear all used caches -w, --watch Enable watch mode (hot reload of generated files) -ff, --fail-fast Stop on first error --version Display the ddb version and check for new ones. Positional argument match command names: run , init , configure , features , config , activate , deactivate . Some optional arguments are available globally , regardless the command. These are placed before the command name . Some commands support additional arguments that can be listed with --help flag after the command name. Those are placed after the command name . ddb config additional arguments ddb config --help ddb config -- help usage : ddb config [ - h ] [ -- variables ] optional arguments : - h , -- help show this help message and exit -- variables Output as a flat list of variables available in template engines Watch mode When setting up a project, you have to execute ddb configure many times while trying to configure the project environment. ddb provides a --watch flag to enable Watch mode. ddb --watch configure ddb -w configure The command will run forever and listen for system file events to perform actions. Available commands \u00b6 ddb init TODO ddb self-update If ddb is installed with the standalone binary and a new version is available on github, it will automatically download it and update the current binary. ddb features This action allows you to check the list of enabled features with a short explanation of what they do. ddb config Display the effective configuration after merge of all configuration files from possible locations. Read more: Configuration Use --variables to check what is available in template engines ddb effective configuration is used as context inside all template engine processing (Jinja, jsonnet, ytt, ...) When working with templates, you might want to include some configuration values into the template. You can use --variables option to display the whole configuration as a flat and dotted notation, as it can be tedious to retrieve the full variable name from the default yaml output. ddb configure Configure the project by scanning project files and performing actions supported by all features. Use --eject to convert the project to a static version --eject option can be used to convert the project to a static version and detach it from ddb. Templates files are removed after generating destination files. It can be used to distribute sources of your project targeting current configuration only. You may also also set docker.jsonnet.virtualhost_disabled and docker.jsonnet.binary_disabled to True to remove jsonnet virtualhosts and binaries from generated docker-compose.yml. DDB_OVERRIDE_JSONNET_DOCKER_VIRTUALHOST_DISABLED = 1 \\ DDB_OVERRIDE_JSONNET_DOCKER_BINARY_DISABLED = 1 \\ ddb configure --eject Deprecated configuration properties and --autofix As ddb evolves during time, some settings and features may become deprecated. When your project use some deprecated configuration property, a warning is displayed like this one. If you are referencing some deprecated configuration keys inside template files, like jsonnet, jinja or ytt, you can run ddb configure --autofix to automatically migrate your template sources. Keep in mind that running this command will make your project future-proof, but can break things for users running older ddb versions. ddb info Displays compacted information about docker containers such as environment variables, virtual host, exposed ports and binaries in a more readable way. +-----------------------------------------------+ | db | +-----------------------------------------------+ | MYSQL_DATABASE : ddb | | MYSQL_PASSWORD : ddb | | MYSQL_ROOT_PASSWORD : ddb | | MYSQL_USER : ddb | +-----------------------------------------------+ | 37306 -> 3306 | +-----------------------------------------------+ | mysql | | mysqldump | +-----------------------------------------------+ Tip: Use --type to filter for a type of information For instance, you want to see only virtual hosts information. Instead of displaying every section, by writing ddb info --type bin you will be provided with a filtered result. You can choose between bin, env, port and vhost +-----------------------------------------------+ | db | +-----------------------------------------------+ | Binaries: | | | | mysql | | mysqldump | +-----------------------------------------------+ ddb activate Display a script for the configured shell that must be evaluated to active the project environment inside the current shell session. Read more: Shell feature ddb deactivate Display a script for the configured shell that must be evaluated to deactivate the project environment inside the current shell session. Read more: Shell feature ddb check-activated Check if project is activated in current shell. Read more: Shell feature ddb run Display the command line to evaluate in the configured shell to run the registered binary.","title":"Commands"},{"location":"commands/#commands","text":"If you run ddb with no argument, the usage is displayed. ddb usage usage: ddb [-h] [-v] [-vv] [-s] [-x] [-c] [-w] [-ff] [--version] {init,configure,features,config,info,self-update,run,activate,deactivate,check-activated} ... positional arguments: {init,configure,features,config,info,self-update,run,activate,deactivate,check-activated} Available commands init Initialize the environment configure Configure the environment features List enabled features config Display effective configuration info Display useful information self-update Update ddb to latest version run Display command to run project binary activate Write a shell script to be executed to activate environment deactivate Write a shell script to be executed to deactivate environment check-activated Check if project is activated in current shell optional arguments: -h, --help show this help message and exit -v, --verbose Enable more logs -vv, --very-verbose Enable even more logs -s, --silent Disable all logs -x, --exceptions Display exceptions on errors -c, --clear-cache Clear all used caches -w, --watch Enable watch mode (hot reload of generated files) -ff, --fail-fast Stop on first error --version Display the ddb version and check for new ones. Positional argument match command names: run , init , configure , features , config , activate , deactivate . Some optional arguments are available globally , regardless the command. These are placed before the command name . Some commands support additional arguments that can be listed with --help flag after the command name. Those are placed after the command name . ddb config additional arguments ddb config --help ddb config -- help usage : ddb config [ - h ] [ -- variables ] optional arguments : - h , -- help show this help message and exit -- variables Output as a flat list of variables available in template engines Watch mode When setting up a project, you have to execute ddb configure many times while trying to configure the project environment. ddb provides a --watch flag to enable Watch mode. ddb --watch configure ddb -w configure The command will run forever and listen for system file events to perform actions.","title":"Commands"},{"location":"commands/#available-commands","text":"ddb init TODO ddb self-update If ddb is installed with the standalone binary and a new version is available on github, it will automatically download it and update the current binary. ddb features This action allows you to check the list of enabled features with a short explanation of what they do. ddb config Display the effective configuration after merge of all configuration files from possible locations. Read more: Configuration Use --variables to check what is available in template engines ddb effective configuration is used as context inside all template engine processing (Jinja, jsonnet, ytt, ...) When working with templates, you might want to include some configuration values into the template. You can use --variables option to display the whole configuration as a flat and dotted notation, as it can be tedious to retrieve the full variable name from the default yaml output. ddb configure Configure the project by scanning project files and performing actions supported by all features. Use --eject to convert the project to a static version --eject option can be used to convert the project to a static version and detach it from ddb. Templates files are removed after generating destination files. It can be used to distribute sources of your project targeting current configuration only. You may also also set docker.jsonnet.virtualhost_disabled and docker.jsonnet.binary_disabled to True to remove jsonnet virtualhosts and binaries from generated docker-compose.yml. DDB_OVERRIDE_JSONNET_DOCKER_VIRTUALHOST_DISABLED = 1 \\ DDB_OVERRIDE_JSONNET_DOCKER_BINARY_DISABLED = 1 \\ ddb configure --eject Deprecated configuration properties and --autofix As ddb evolves during time, some settings and features may become deprecated. When your project use some deprecated configuration property, a warning is displayed like this one. If you are referencing some deprecated configuration keys inside template files, like jsonnet, jinja or ytt, you can run ddb configure --autofix to automatically migrate your template sources. Keep in mind that running this command will make your project future-proof, but can break things for users running older ddb versions. ddb info Displays compacted information about docker containers such as environment variables, virtual host, exposed ports and binaries in a more readable way. +-----------------------------------------------+ | db | +-----------------------------------------------+ | MYSQL_DATABASE : ddb | | MYSQL_PASSWORD : ddb | | MYSQL_ROOT_PASSWORD : ddb | | MYSQL_USER : ddb | +-----------------------------------------------+ | 37306 -> 3306 | +-----------------------------------------------+ | mysql | | mysqldump | +-----------------------------------------------+ Tip: Use --type to filter for a type of information For instance, you want to see only virtual hosts information. Instead of displaying every section, by writing ddb info --type bin you will be provided with a filtered result. You can choose between bin, env, port and vhost +-----------------------------------------------+ | db | +-----------------------------------------------+ | Binaries: | | | | mysql | | mysqldump | +-----------------------------------------------+ ddb activate Display a script for the configured shell that must be evaluated to active the project environment inside the current shell session. Read more: Shell feature ddb deactivate Display a script for the configured shell that must be evaluated to deactivate the project environment inside the current shell session. Read more: Shell feature ddb check-activated Check if project is activated in current shell. Read more: Shell feature ddb run Display the command line to evaluate in the configured shell to run the registered binary.","title":"Available commands"},{"location":"configuration/","text":"Configuration \u00b6 ddb use yaml configuration files : ddb.yml , the main configuration file ddb.<env>.yml , the environment configuration file override, where <env> can be dev , stage , ci or prod . ddb.local.yml , the configuration file local override. Those files can be placed in those directories : Inside ~/.docker-devbox/ddb Inside ~/.docker-devbox Inside the project directory root If many configuration files are found, they are merged with given filenames and directories order. This configuration is used internally by feature actions, but is also injected as context for each supported template engine . You can add any data structure inside the configuration file so this data is available inside template engines. How to check effective configuration You can run ddb config to view the merged effective configuration. Each feature holds it's own configuration section under the name of the feature . For details about supported configuration settings, please check the documentation of related feature. Default merge behavior By default, when ddb merge a configuration file, objects are be deeply merged, but any other data type is overriden. ddb.yml docker : disabled_services : [ 'python' ] ddb.local.yml docker : disabled_services : [] effective configuration (ddb config) docker : disabled_services : [] As you can see lists are overriden by default too. Custom merge behavior You can specify custom merge behavior using an object containing two properties value : The actual value to merge merge : The merge strategy to apply For lists, you may use the following merge strategies: override (default) append prepend insert append_if_missing prepend_if_missing insert_if_missing For objects, you may use the following merge strategies: merge (default) override ddb.yml docker : disabled_services : [ 'python' ] ddb.local.yml docker : disabled_services : merge : append_if_missing value : [ 'gunicorn' ] effective configuration (ddb config) docker : disabled_services : [ 'python' , 'gunicorn' ] Configuration and environment variables To override any configuration setting, you may set an environment variable. To convert the property name from dotted notation to environment variable, you should write in UPPERCASE, add DDB_ prefix replace . with _ . For instance, core.domain.ext can be overriden with DDB_CORE_DOMAIN_EXT environment variable. Variables lists can be referenced with [0] , [1] , ..., [n] When activating the environment $(ddb activate) with shell feature (Environment activation) , the whole configuration is also exported as environment variables using the same naming convention. Extra configuration files You may add additional configuration files using core.configuration.extra_files configuration property into default configuration files. core: configuration: extra: ['ddb.custom.yml'] This will load ddb.custom.yml configuration file from each supported configuration directories. ddb.local.yml still has the priority other those extra configuration files. Default configuration ddb config can provide default configuration if ddb.yml configuration file is empty. binary: disabled: false certs: cfssl: append_ca_certificate: true server: host: localhost port: 7780 ssl: false verify_cert: true writer: filenames: certificate: '%s.crt' certificate_der: '%s.crt.der' certificate_request: '%s.csr' certificate_request_der: '%s.csr.der' private_key: '%s.key' destination: .certs disabled: false type: cfssl copy: disabled: false core: disabled: false domain: ext: test sub: ddb-quickstart env: available: - prod - stage - ci - dev current: dev os: posix path: ddb_home: /home/toilal/.docker-devbox/ddb home: /home/toilal/.docker-devbox project_home: /home/toilal/ddb-quickstart process: {} project: name: ddb-quickstart docker: build_image_tag: master build_image_tag_from_version: true cache_from_image: false compose: network_name: ddb-quickstart_default project_name: ddb-quickstart debug: disabled: false host: 172.17.0.1 directory: .docker disabled: false interface: docker0 ip: 172.17.0.1 path_mapping: {} port_prefix: 507 registry: name: null repository: null restart_policy: 'no' reverse_proxy: certresolver: null network_id: reverse-proxy network_names: reverse-proxy: reverse-proxy redirect_to_https: null type: traefik user: gid: 1000 uid: 1000 file: disabled: false excludes: - '**/_*' - '**/.git' - '**/node_modules' - '**/vendor' fixuid: disabled: false url: https://github.com/boxboat/fixuid/releases/download/v0.4/fixuid-0.4-linux-amd64.tar.gz git: disabled: false fix_files_permissions: true gitignore: disabled: false jinja: disabled: false extensions: - .* - '' includes: - '*.jinja{.*,}' suffixes: - .jinja jsonnet: disabled: false extensions: - .* - '' includes: - '*.jsonnet{.*,}' suffixes: - .jsonnet permissions: disabled: false specs: null shell: aliases: {} disabled: false envignore: - PYENV_* - _ - PS1 - PS2 - PS3 - PS4 path: directories: - .bin - bin prepend: true shell: bash smartcd: disabled: true symlinks: disabled: false includes: - '*.dev{.*,}' suffixes: - .dev traefik: certs_directory: /home/toilal/.docker-devbox/certs config_directory: /home/toilal/.docker-devbox/traefik/config disabled: false mapped_certs_directory: /certs ssl_config_template: \"# This configuration file has been automatically generated\\ \\ by ddb\\n[[tls.certificates]]\\n certFile = \\\"%s\\\"\\n keyFile = \\\"%s\\\"\\n\" version: branch: master disabled: false hash: e78bccab1e980244c31592a923902525ede8985b short_hash: e78bcca tag: null version: null ytt: args: - --ignore-unknown-comments depends_suffixes: - .data - .overlay disabled: false extensions: - .yaml - .yml - '' includes: - '*.ytt{.yaml,.yml,}' keywords: - and - elif - in - or - break - else - lambda - pass - continue - for - load - return - def - if - not - while - as - finally - nonlocal - assert - from - raise - class - global - try - del - import - with - except - is - yield keywords_escape_format: '%s_' suffixes: - .ytt","title":"Configuration"},{"location":"configuration/#configuration","text":"ddb use yaml configuration files : ddb.yml , the main configuration file ddb.<env>.yml , the environment configuration file override, where <env> can be dev , stage , ci or prod . ddb.local.yml , the configuration file local override. Those files can be placed in those directories : Inside ~/.docker-devbox/ddb Inside ~/.docker-devbox Inside the project directory root If many configuration files are found, they are merged with given filenames and directories order. This configuration is used internally by feature actions, but is also injected as context for each supported template engine . You can add any data structure inside the configuration file so this data is available inside template engines. How to check effective configuration You can run ddb config to view the merged effective configuration. Each feature holds it's own configuration section under the name of the feature . For details about supported configuration settings, please check the documentation of related feature. Default merge behavior By default, when ddb merge a configuration file, objects are be deeply merged, but any other data type is overriden. ddb.yml docker : disabled_services : [ 'python' ] ddb.local.yml docker : disabled_services : [] effective configuration (ddb config) docker : disabled_services : [] As you can see lists are overriden by default too. Custom merge behavior You can specify custom merge behavior using an object containing two properties value : The actual value to merge merge : The merge strategy to apply For lists, you may use the following merge strategies: override (default) append prepend insert append_if_missing prepend_if_missing insert_if_missing For objects, you may use the following merge strategies: merge (default) override ddb.yml docker : disabled_services : [ 'python' ] ddb.local.yml docker : disabled_services : merge : append_if_missing value : [ 'gunicorn' ] effective configuration (ddb config) docker : disabled_services : [ 'python' , 'gunicorn' ] Configuration and environment variables To override any configuration setting, you may set an environment variable. To convert the property name from dotted notation to environment variable, you should write in UPPERCASE, add DDB_ prefix replace . with _ . For instance, core.domain.ext can be overriden with DDB_CORE_DOMAIN_EXT environment variable. Variables lists can be referenced with [0] , [1] , ..., [n] When activating the environment $(ddb activate) with shell feature (Environment activation) , the whole configuration is also exported as environment variables using the same naming convention. Extra configuration files You may add additional configuration files using core.configuration.extra_files configuration property into default configuration files. core: configuration: extra: ['ddb.custom.yml'] This will load ddb.custom.yml configuration file from each supported configuration directories. ddb.local.yml still has the priority other those extra configuration files. Default configuration ddb config can provide default configuration if ddb.yml configuration file is empty. binary: disabled: false certs: cfssl: append_ca_certificate: true server: host: localhost port: 7780 ssl: false verify_cert: true writer: filenames: certificate: '%s.crt' certificate_der: '%s.crt.der' certificate_request: '%s.csr' certificate_request_der: '%s.csr.der' private_key: '%s.key' destination: .certs disabled: false type: cfssl copy: disabled: false core: disabled: false domain: ext: test sub: ddb-quickstart env: available: - prod - stage - ci - dev current: dev os: posix path: ddb_home: /home/toilal/.docker-devbox/ddb home: /home/toilal/.docker-devbox project_home: /home/toilal/ddb-quickstart process: {} project: name: ddb-quickstart docker: build_image_tag: master build_image_tag_from_version: true cache_from_image: false compose: network_name: ddb-quickstart_default project_name: ddb-quickstart debug: disabled: false host: 172.17.0.1 directory: .docker disabled: false interface: docker0 ip: 172.17.0.1 path_mapping: {} port_prefix: 507 registry: name: null repository: null restart_policy: 'no' reverse_proxy: certresolver: null network_id: reverse-proxy network_names: reverse-proxy: reverse-proxy redirect_to_https: null type: traefik user: gid: 1000 uid: 1000 file: disabled: false excludes: - '**/_*' - '**/.git' - '**/node_modules' - '**/vendor' fixuid: disabled: false url: https://github.com/boxboat/fixuid/releases/download/v0.4/fixuid-0.4-linux-amd64.tar.gz git: disabled: false fix_files_permissions: true gitignore: disabled: false jinja: disabled: false extensions: - .* - '' includes: - '*.jinja{.*,}' suffixes: - .jinja jsonnet: disabled: false extensions: - .* - '' includes: - '*.jsonnet{.*,}' suffixes: - .jsonnet permissions: disabled: false specs: null shell: aliases: {} disabled: false envignore: - PYENV_* - _ - PS1 - PS2 - PS3 - PS4 path: directories: - .bin - bin prepend: true shell: bash smartcd: disabled: true symlinks: disabled: false includes: - '*.dev{.*,}' suffixes: - .dev traefik: certs_directory: /home/toilal/.docker-devbox/certs config_directory: /home/toilal/.docker-devbox/traefik/config disabled: false mapped_certs_directory: /certs ssl_config_template: \"# This configuration file has been automatically generated\\ \\ by ddb\\n[[tls.certificates]]\\n certFile = \\\"%s\\\"\\n keyFile = \\\"%s\\\"\\n\" version: branch: master disabled: false hash: e78bccab1e980244c31592a923902525ede8985b short_hash: e78bcca tag: null version: null ytt: args: - --ignore-unknown-comments depends_suffixes: - .data - .overlay disabled: false extensions: - .yaml - .yml - '' includes: - '*.ytt{.yaml,.yml,}' keywords: - and - elif - in - or - break - else - lambda - pass - continue - for - load - return - def - if - not - while - as - finally - nonlocal - assert - from - raise - class - global - try - del - import - with - except - is - yield keywords_escape_format: '%s_' suffixes: - .ytt","title":"Configuration"},{"location":"faq/","text":"Frequently Asked Questions \u00b6 How to customize the configuration for a single project ? You may create a ddb.local.yml configuration file in the project directory and add it to .gitignore . Read more: Configuration How to customize the configuration for all project on the host ? You may create a ddb.yml configuration file in the ~/.docker-devbox installation directory. You can also add environment variable matching the property name to override, in UPPERCASE, prefixed with DDB_ and . replaced with _ . Read more: Configuration How to change the domain name for a single project ? Domain name is configured through 2 core settings. core.domain.ext: test core.domain.sub: folder Those settings are joined with . to build the main domain name ( folder.test ). Read more: Configuration How to change the domain extension for all projects ? Domain name is configured through 2 core settings. core.domain.ext: test core.domain.sub: folder Those settings are joined with . , folder.test to build the main domain name. You can override core.domain.ext setting globally by creating a ddb.local.yml file in ~/.docker-devbox installation directory. You can also define a system environment variable named DDB_CORE_DOMAIN_EXT with the domain extension. Read more: Configuration How to disable/enable tags from docker images defined in generated docker-compose.yml file ? In your project ddb.yml file, you may disable this option in the docker feature with docker.build_image_tag_from_version set to false. Read more: Configuration CI fails with the following message: \"the input device is not a TTY\" If no TTY is available, you have to set the following environment variable to workaround this issue COMPOSE_INTERACTIVE_NO_CLI=1","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"How to customize the configuration for a single project ? You may create a ddb.local.yml configuration file in the project directory and add it to .gitignore . Read more: Configuration How to customize the configuration for all project on the host ? You may create a ddb.yml configuration file in the ~/.docker-devbox installation directory. You can also add environment variable matching the property name to override, in UPPERCASE, prefixed with DDB_ and . replaced with _ . Read more: Configuration How to change the domain name for a single project ? Domain name is configured through 2 core settings. core.domain.ext: test core.domain.sub: folder Those settings are joined with . to build the main domain name ( folder.test ). Read more: Configuration How to change the domain extension for all projects ? Domain name is configured through 2 core settings. core.domain.ext: test core.domain.sub: folder Those settings are joined with . , folder.test to build the main domain name. You can override core.domain.ext setting globally by creating a ddb.local.yml file in ~/.docker-devbox installation directory. You can also define a system environment variable named DDB_CORE_DOMAIN_EXT with the domain extension. Read more: Configuration How to disable/enable tags from docker images defined in generated docker-compose.yml file ? In your project ddb.yml file, you may disable this option in the docker feature with docker.build_image_tag_from_version set to false. Read more: Configuration CI fails with the following message: \"the input device is not a TTY\" If no TTY is available, you have to set the following environment variable to workaround this issue COMPOSE_INTERACTIVE_NO_CLI=1","title":"Frequently Asked Questions"},{"location":"overview/","text":"Overview \u00b6 Technology agnostic \u00b6 ddb is not tied to a particular language, framework or technical stack. It's designed to automate application and docker-compose configuration on an environment, so you forget about the docker hard stuff while writing code, and so application can be deployed to stage and production environment with no changes on your sources. Docker Devbox \u00b6 Even if ddb can be used as a standalone tool, it has been designed with docker and docker-compose in mind . Many features requires some containers to run in background. Those containers have been configured and packaged in docker-devbox , so you should really consider to install it. docker-devbox brings the following containers: traefik , to automatically proxy services provided by docker-compose using project domain name virtualhost on HTTP/80 and HTTPS/443 , and generate SSL certificates with Letsencrypt (for public domain names). cfssl , the Cloudflare's PKI and TLS toolkit, to generate certificates for internal domains . portainer to manage containers from a web browser. Please read docker-devbox README to perform the installation properly. Eat your own dog food docker-devbox containers are configured with ddb themselves. Features, actions and events \u00b6 ddb make use of features to perform it's magic. Each feature holds it's own configuration and embeds at least one action . An action brings at least one binding between an event and a function , so when this event occurs the function is called. An action's function implementation can also raise other events that will trigger other feature action functions, making ddb a fully reactive software. What occurs when running ddb ? When running ddb configure command, a bunch of events are triggered. Firstly, the command raise the phase.configure event, which is binded to the file feature . Secondly, file feature scans files and triggers file:found event for each file in the project directory. Thirdly, the jsonnet feature, which is binded to file:found event with a filter to match .jsonnet file extension only, so that only those files are processed by the jsonnet template engine. Other features perform actions the same way, like jinja , symlinks and ytt features. Those actions raises other events, like file:generated for each generated files, that will be processed by the gitignore feature to add generated files to ignore list. And so on.","title":"Overview"},{"location":"overview/#overview","text":"","title":"Overview"},{"location":"overview/#technology-agnostic","text":"ddb is not tied to a particular language, framework or technical stack. It's designed to automate application and docker-compose configuration on an environment, so you forget about the docker hard stuff while writing code, and so application can be deployed to stage and production environment with no changes on your sources.","title":"Technology agnostic"},{"location":"overview/#docker-devbox","text":"Even if ddb can be used as a standalone tool, it has been designed with docker and docker-compose in mind . Many features requires some containers to run in background. Those containers have been configured and packaged in docker-devbox , so you should really consider to install it. docker-devbox brings the following containers: traefik , to automatically proxy services provided by docker-compose using project domain name virtualhost on HTTP/80 and HTTPS/443 , and generate SSL certificates with Letsencrypt (for public domain names). cfssl , the Cloudflare's PKI and TLS toolkit, to generate certificates for internal domains . portainer to manage containers from a web browser. Please read docker-devbox README to perform the installation properly. Eat your own dog food docker-devbox containers are configured with ddb themselves.","title":"Docker Devbox"},{"location":"overview/#features-actions-and-events","text":"ddb make use of features to perform it's magic. Each feature holds it's own configuration and embeds at least one action . An action brings at least one binding between an event and a function , so when this event occurs the function is called. An action's function implementation can also raise other events that will trigger other feature action functions, making ddb a fully reactive software. What occurs when running ddb ? When running ddb configure command, a bunch of events are triggered. Firstly, the command raise the phase.configure event, which is binded to the file feature . Secondly, file feature scans files and triggers file:found event for each file in the project directory. Thirdly, the jsonnet feature, which is binded to file:found event with a filter to match .jsonnet file extension only, so that only those files are processed by the jsonnet template engine. Other features perform actions the same way, like jinja , symlinks and ytt features. Those actions raises other events, like file:generated for each generated files, that will be processed by the gitignore feature to add generated files to ignore list. And so on.","title":"Features, actions and events"},{"location":"templates/","text":"Templates \u00b6 ddb provides some template engine support, like Jinja , Jsonnet and ytt . Each template engine is implemented in it's own feature, jinja , jsonnet and ytt . Files are automatically processed Files are automatically processed by a template engine based on file extension. Dockerfile.jinja is processed by jinja and produces Dockerfile file. The template filename extension can also be found when placed just before the target filename extension. Both data.json.jinja and data.jinja.json is processed by jinja and produces data.json . Automate your deployments for all environments Templates are perfect for configuration files that should change for each environment, or to embed some global configuration settings into many static configuration files. You should abuse of templates to make deployment on various environment a breeze, as the application will auto-configure based on ddb configuration of each environment. git clone , ddb configure and that's all ! Your application is ready to run on dev, stage or production environment. Keep in mind that you can freely add any setting inside ddb.yml project configuration , it so you can retrieve them from any template. Jinja \u00b6 Jinja is a modern and designer-friendly templating language, modelled after Django\u2019s templates. It's a general purpose templating engine that doesn't target any particular file format. Write Dockerfile.jinja instead of raw Dockerfile Jinja is really handy to make your Dockerfile dynamic. Writing Dockerfile.jinja files instead of raw Dockerfile is recommended in ddb, and mandatory when using fixuid feature . Jsonnet \u00b6 Jsonnet is a data templating language for app and tool developers. It's an extension of json and can only output JSON . In ddb, Jsonnet mostly works like others template engines, but the extension of the target file is used to guess the target output format: JSON or YAML . As Jsonnet can only output JSON , YAML output is converted from JSON thanks to python environment. docker-compose.yml.jsonnet is processed by jsonnet and produces a yaml as output. data.jsonnet.json is processed by jsonnet and produces json as output. Jsonnet produces json as default output By default, jsonnet produces json as output, unless extension of the target file is .yml . Write docker-compose.yml.jsonnet instead of raw docker-compose.yml ddb brings many docker and docker-compose features through jsonnet, so you'd better writing a docker-compose.yml.jsonnet file instead of a raw docker-compose.yml configuration. Read more: jsonnet Feature (Docker-compose jsonnet library) ytt \u00b6 ytt is a templating tool that understands YAML structure allowing you to focus on your data instead of how to properly escape it. Ytt templates are a superset of YAML and can only generate yaml files. You may like ytt , but you'd better focus on learning Jsonnet Because Jsonnet supports custom functions, and both JSON et YAML output, we consider ytt as a secondary template engine. It's available and fully supported in ddb, so you can use it to template any YAML configuration files inside your application. But if you implement docker-compose.yml.ytt , you won't be able to use all features available in ddb docker-compose jsonnet library. Read more: jsonnet Feature (Docker-compose jsonnet library) Symlinks \u00b6 Symlinks feature allow to create a symlink from many possible files, each source file matching a supported environment. Even if it's not based on a template engine, symlinks feature shares some of behavior from other template based features as it generates a symlink from another file. By default, ddb holds the following configuration settings: core.env.available : [ 'prod' , 'stage' , 'ci' , 'dev' ] core.env.current : dev core.env.available contains all supported environment values core.env.current match the actual environment. Consider a project with a configuration file named settings.yaml . This file should be different for each environment. Thanks to ddb and without implementing custom configuration logic inside the application, you can create a file for each environment, and the symlink matching the current environment is generated. settings.yaml.prod -> Settings for prod environment settings.yaml.stage -> Settings for stage environment settings.yaml.dev -> Settings for dev environment settings.yaml -> Symlink pointing to file based on core.env.current If core.env.current is set to dev , settings.yaml symlink points to settings.yaml.dev If core.env.current is set to stage , settings.yaml symlink points to settings.yaml.stage If core.env.current is set to prod , settings.yaml symlink points to settings.yaml.prod What if no file exists for the current environment ? There's no settings.yaml.ci file, but if core.env.current is set to ci , settings.yaml symlink will still point to settings.yaml.dev The fallback behavoir is to find the first existing file on the right of ci in core.env.available . If settings.yaml.dev doesn't exists, no symlink is created at all. So now, you can refer to this symlink to load the configuration inside your application, so it will switch automatically when deploying on stage or prod environment. Where to use a symlink, where to use a template ? Use a symlink where changes are conditionned by core.env.current environment setting and affects most of the configuration file content. Use a template where changes are conditionned by any other environnement setting, or when changes affects a very small portion of the configuration file content. Anyway, You can use both symlink and template to generate a single configuration file as ddb is reactive and supports natural action chaining. You can create a symlink first, and then resolve a template. settings.yaml.ytt.prod -> Settings template for prod environment settings.yaml.ytt.stage -> Settings template for stage environment settings.yaml.ytt.dev -> Settings template for dev environment settings.yaml.ytt -> Symlink pointing to settings template file based on core.env.current settings.yaml -> Generated settings file from Symlink through ytt template engine Or you can resolve a template first, and then create a symlink. settings.yaml.prod.ytt -> Settings template for prod environment settings.yaml.stage.ytt -> Settings template for stage environment settings.yaml.dev.ytt -> Settings template for dev environment settings.yaml.prod -> Generated settings file for prod environment settings.yaml.stage -> Generated settings file for stage environment settings.yaml.dev -> Generated settings file for dev environment settings.yaml -> Symlink pointing to generated settings file based on core.env.current","title":"Templates"},{"location":"templates/#templates","text":"ddb provides some template engine support, like Jinja , Jsonnet and ytt . Each template engine is implemented in it's own feature, jinja , jsonnet and ytt . Files are automatically processed Files are automatically processed by a template engine based on file extension. Dockerfile.jinja is processed by jinja and produces Dockerfile file. The template filename extension can also be found when placed just before the target filename extension. Both data.json.jinja and data.jinja.json is processed by jinja and produces data.json . Automate your deployments for all environments Templates are perfect for configuration files that should change for each environment, or to embed some global configuration settings into many static configuration files. You should abuse of templates to make deployment on various environment a breeze, as the application will auto-configure based on ddb configuration of each environment. git clone , ddb configure and that's all ! Your application is ready to run on dev, stage or production environment. Keep in mind that you can freely add any setting inside ddb.yml project configuration , it so you can retrieve them from any template.","title":"Templates"},{"location":"templates/#jinja","text":"Jinja is a modern and designer-friendly templating language, modelled after Django\u2019s templates. It's a general purpose templating engine that doesn't target any particular file format. Write Dockerfile.jinja instead of raw Dockerfile Jinja is really handy to make your Dockerfile dynamic. Writing Dockerfile.jinja files instead of raw Dockerfile is recommended in ddb, and mandatory when using fixuid feature .","title":"Jinja"},{"location":"templates/#jsonnet","text":"Jsonnet is a data templating language for app and tool developers. It's an extension of json and can only output JSON . In ddb, Jsonnet mostly works like others template engines, but the extension of the target file is used to guess the target output format: JSON or YAML . As Jsonnet can only output JSON , YAML output is converted from JSON thanks to python environment. docker-compose.yml.jsonnet is processed by jsonnet and produces a yaml as output. data.jsonnet.json is processed by jsonnet and produces json as output. Jsonnet produces json as default output By default, jsonnet produces json as output, unless extension of the target file is .yml . Write docker-compose.yml.jsonnet instead of raw docker-compose.yml ddb brings many docker and docker-compose features through jsonnet, so you'd better writing a docker-compose.yml.jsonnet file instead of a raw docker-compose.yml configuration. Read more: jsonnet Feature (Docker-compose jsonnet library)","title":"Jsonnet"},{"location":"templates/#ytt","text":"ytt is a templating tool that understands YAML structure allowing you to focus on your data instead of how to properly escape it. Ytt templates are a superset of YAML and can only generate yaml files. You may like ytt , but you'd better focus on learning Jsonnet Because Jsonnet supports custom functions, and both JSON et YAML output, we consider ytt as a secondary template engine. It's available and fully supported in ddb, so you can use it to template any YAML configuration files inside your application. But if you implement docker-compose.yml.ytt , you won't be able to use all features available in ddb docker-compose jsonnet library. Read more: jsonnet Feature (Docker-compose jsonnet library)","title":"ytt"},{"location":"templates/#symlinks","text":"Symlinks feature allow to create a symlink from many possible files, each source file matching a supported environment. Even if it's not based on a template engine, symlinks feature shares some of behavior from other template based features as it generates a symlink from another file. By default, ddb holds the following configuration settings: core.env.available : [ 'prod' , 'stage' , 'ci' , 'dev' ] core.env.current : dev core.env.available contains all supported environment values core.env.current match the actual environment. Consider a project with a configuration file named settings.yaml . This file should be different for each environment. Thanks to ddb and without implementing custom configuration logic inside the application, you can create a file for each environment, and the symlink matching the current environment is generated. settings.yaml.prod -> Settings for prod environment settings.yaml.stage -> Settings for stage environment settings.yaml.dev -> Settings for dev environment settings.yaml -> Symlink pointing to file based on core.env.current If core.env.current is set to dev , settings.yaml symlink points to settings.yaml.dev If core.env.current is set to stage , settings.yaml symlink points to settings.yaml.stage If core.env.current is set to prod , settings.yaml symlink points to settings.yaml.prod What if no file exists for the current environment ? There's no settings.yaml.ci file, but if core.env.current is set to ci , settings.yaml symlink will still point to settings.yaml.dev The fallback behavoir is to find the first existing file on the right of ci in core.env.available . If settings.yaml.dev doesn't exists, no symlink is created at all. So now, you can refer to this symlink to load the configuration inside your application, so it will switch automatically when deploying on stage or prod environment. Where to use a symlink, where to use a template ? Use a symlink where changes are conditionned by core.env.current environment setting and affects most of the configuration file content. Use a template where changes are conditionned by any other environnement setting, or when changes affects a very small portion of the configuration file content. Anyway, You can use both symlink and template to generate a single configuration file as ddb is reactive and supports natural action chaining. You can create a symlink first, and then resolve a template. settings.yaml.ytt.prod -> Settings template for prod environment settings.yaml.ytt.stage -> Settings template for stage environment settings.yaml.ytt.dev -> Settings template for dev environment settings.yaml.ytt -> Symlink pointing to settings template file based on core.env.current settings.yaml -> Generated settings file from Symlink through ytt template engine Or you can resolve a template first, and then create a symlink. settings.yaml.prod.ytt -> Settings template for prod environment settings.yaml.stage.ytt -> Settings template for stage environment settings.yaml.dev.ytt -> Settings template for dev environment settings.yaml.prod -> Generated settings file for prod environment settings.yaml.stage -> Generated settings file for stage environment settings.yaml.dev -> Generated settings file for dev environment settings.yaml -> Symlink pointing to generated settings file based on core.env.current","title":"Symlinks"},{"location":"features/certs/","text":"Certs \u00b6 For some project, we needed to have SSL activated for HTTPS access of our projects. So, SSL certificates became a project dependency. The issue was that in local environments, sometimes we cannot open port 80 to the world and get a true Let's Encrypt certificate or don't want to use the native traefik certificate management. One solution we found is to manage certificates using CFSSL to generate and manage a true certificate generation locally. This feature currently handle only CFSSL certificate generation and management. Feature configuration (prefixed with certs. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? type string cfssl Type of certificate generation. Currently, only cfssl is supported. cfssl.server.host boolean localhost CFSSL host (without protocol). cfssl.server.port integer 7780 CFSSL port to connect to. cfssl.server.ssl integer false Should SSL be used to connect ? cfssl.server.verify_cert integer false Should the CFSSL SSL certificate be verified on connect ? Advanced Property Type Description cfssl.verify_checksum integer false Should the CFSSL generated certificates be verified with checksums. cfssl.append_ca_certificate boolean true Should the signer (CA) certificate be appended to the generated certificate ? destination string .certs Destination directory of generated certificates. signer_destinations string[] [] Additional destinations for signer (CA) certificates. Internal Property Type Description cfssl.writer.filenames.certificate string %s.crt Filename template of generated PEM certificates. cfssl.writer.filenames.certificate_der string %s.crt.der Filename template of generated DER certificates. cfssl.writer.filenames.certificate_request string %s.csr Filename template of PEM generated certificate requests. cfssl.writer.filenames.certificate_request_der string %s.csr.der Filename template of name of DER geberated certificate requests. cfssl.writer.filenames.private_key string %s.key Filename template of generated PEM private key. Defaults certs : cfssl : append_ca_certificate : true server : host : localhost port : 7780 ssl : false verify_cert : true verify_checksum : true writer : filenames : certificate : '%s.crt' certificate_der : '%s.crt.der' certificate_request : '%s.csr' certificate_request_der : '%s.csr.der' private_key : '%s.key' destination : .certs signer_destinations : [] disabled : false type : cfssl Certificate generation \u00b6 When running ddb configure command, this action will be triggered. Based on the configuration, the right certificate manager will be used to generate the appropriate certificate in the certs.destination folder if it does not already exist. At the end, it will trigger a certs.generated and certs.available event which can be used by other features to use those generated certificates. For instance, the traefik is listening for the certs.available event in order to update the configuration in order to use them for HTTPS. Certificate removal \u00b6 When running ddb configure command, this action will be triggered. Based on the configuration, if certificates was generated before and the certs.type is switch to something else than cfssl , those certificates will be deleted. At the end, it will trigger a certs.removed event which can be used by other features to update themselves. For instance, the traefik is listening for the certs.removed event in order to update the configuration in order to remove them.","title":"certs"},{"location":"features/certs/#certs","text":"For some project, we needed to have SSL activated for HTTPS access of our projects. So, SSL certificates became a project dependency. The issue was that in local environments, sometimes we cannot open port 80 to the world and get a true Let's Encrypt certificate or don't want to use the native traefik certificate management. One solution we found is to manage certificates using CFSSL to generate and manage a true certificate generation locally. This feature currently handle only CFSSL certificate generation and management. Feature configuration (prefixed with certs. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? type string cfssl Type of certificate generation. Currently, only cfssl is supported. cfssl.server.host boolean localhost CFSSL host (without protocol). cfssl.server.port integer 7780 CFSSL port to connect to. cfssl.server.ssl integer false Should SSL be used to connect ? cfssl.server.verify_cert integer false Should the CFSSL SSL certificate be verified on connect ? Advanced Property Type Description cfssl.verify_checksum integer false Should the CFSSL generated certificates be verified with checksums. cfssl.append_ca_certificate boolean true Should the signer (CA) certificate be appended to the generated certificate ? destination string .certs Destination directory of generated certificates. signer_destinations string[] [] Additional destinations for signer (CA) certificates. Internal Property Type Description cfssl.writer.filenames.certificate string %s.crt Filename template of generated PEM certificates. cfssl.writer.filenames.certificate_der string %s.crt.der Filename template of generated DER certificates. cfssl.writer.filenames.certificate_request string %s.csr Filename template of PEM generated certificate requests. cfssl.writer.filenames.certificate_request_der string %s.csr.der Filename template of name of DER geberated certificate requests. cfssl.writer.filenames.private_key string %s.key Filename template of generated PEM private key. Defaults certs : cfssl : append_ca_certificate : true server : host : localhost port : 7780 ssl : false verify_cert : true verify_checksum : true writer : filenames : certificate : '%s.crt' certificate_der : '%s.crt.der' certificate_request : '%s.csr' certificate_request_der : '%s.csr.der' private_key : '%s.key' destination : .certs signer_destinations : [] disabled : false type : cfssl","title":"Certs"},{"location":"features/certs/#certificate-generation","text":"When running ddb configure command, this action will be triggered. Based on the configuration, the right certificate manager will be used to generate the appropriate certificate in the certs.destination folder if it does not already exist. At the end, it will trigger a certs.generated and certs.available event which can be used by other features to use those generated certificates. For instance, the traefik is listening for the certs.available event in order to update the configuration in order to use them for HTTPS.","title":"Certificate generation"},{"location":"features/certs/#certificate-removal","text":"When running ddb configure command, this action will be triggered. Based on the configuration, if certificates was generated before and the certs.type is switch to something else than cfssl , those certificates will be deleted. At the end, it will trigger a certs.removed event which can be used by other features to update themselves. For instance, the traefik is listening for the certs.removed event in order to update the configuration in order to remove them.","title":"Certificate removal"},{"location":"features/copy/","text":"Copy \u00b6 The copy feature is a simple way to automatically copy files from one folder to another. This feature is bounded to the init command . Feature configuration (prefixed with copy. ) Property Type Description disabled boolean false Should this feature be disabled ? specs Spec [] List of specifications of files/patterns to copy Spec configuration (used in copy.specs ) Property Type Description source string * The source file to copy, or a glob expression matching files to copy. it can be a local file path, or it can starts with http(s):// to copy from a remote URL. destination string . The exact target destination file or directory. filename string The destination filename. If empty and destination match a directory, source filename will be used. dispatch string[] A list of directories or directory globs where the file will be duplicated. i.e if set to ['target'] , source file with be copied to target directory using filename defined in destination property. If set to ['target/* ], it will be copied in each subdirectory of target directory using filename defined in destination` property. Defaults copy : disabled : false Copy a file from an URL copy : specs : - source : 'https://github.com/boxboat/fixuid/releases/download/v0.5/fixuid-0.5-linux-amd64.tar.gz' destination : '.docker' filename : 'fixuid.tar.gz' Copy many files from filesystem copy : specs : - source : '/etc/ssl/certs/*' destination : 'host-certs'","title":"copy"},{"location":"features/copy/#copy","text":"The copy feature is a simple way to automatically copy files from one folder to another. This feature is bounded to the init command . Feature configuration (prefixed with copy. ) Property Type Description disabled boolean false Should this feature be disabled ? specs Spec [] List of specifications of files/patterns to copy Spec configuration (used in copy.specs ) Property Type Description source string * The source file to copy, or a glob expression matching files to copy. it can be a local file path, or it can starts with http(s):// to copy from a remote URL. destination string . The exact target destination file or directory. filename string The destination filename. If empty and destination match a directory, source filename will be used. dispatch string[] A list of directories or directory globs where the file will be duplicated. i.e if set to ['target'] , source file with be copied to target directory using filename defined in destination property. If set to ['target/* ], it will be copied in each subdirectory of target directory using filename defined in destination` property. Defaults copy : disabled : false Copy a file from an URL copy : specs : - source : 'https://github.com/boxboat/fixuid/releases/download/v0.5/fixuid-0.5-linux-amd64.tar.gz' destination : '.docker' filename : 'fixuid.tar.gz' Copy many files from filesystem copy : specs : - source : '/etc/ssl/certs/*' destination : 'host-certs'","title":"Copy"},{"location":"features/core/","text":"Core \u00b6 Core feature contains some key configuration like domain name and current active environement. Those configuration settings may impact many other features indirectly. It also handle the two following basic commands : ddb features and ddb config . Check command page for details about those commands. Feature configuration (prefixed with core. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? domain.sub string ${project.name} The domain to use for the domain generation. This is the constant part of the domain, that should not vary between environment. domain.ext string test The extension to use for the domain. This is the last part, of your domain name, that may vary between environment. env.current string ${env.available}[-1] Current active environment. Default value is dev , or the last value of env.available . project.name string <Directory name of ${path.project_home}> The project name. This is used by many templates and to generate other default values like domain.sub . Advanced Property Type Description env.available string[] ['prod', 'stage', 'ci', 'dev'] List of available environments. You should any new custom environment to support here before trying to set env.current to this custom environment. required_version string Minimal required ddb version for the project to work properly. If required_version is greater than the currently running one, ddb will refuse to run until it's updated. path.ddb_home string ${env:HOME}/.docker-devbox/ddb The path where ddb is installed. path.home string ${env:HOME}/.docker-devbox The path where docker devbox is installed. path.project_home string The project directory. process Process [] List of process configurations. A process configuration allow to add custom arguments before and after a command executed internally (like git ). Internal Property Type Description os string posix The current operating system. github_repository string gfi-centre-ouest/docker-devbox-ddb List of process configurations. A process configuration allow to add flags before and after a command normaly runned by ddb (like git ). Process configuration (used in core.process ) Property Type Description bin string * The process path to override. prepend string|string[] Arguments to prepend to default arguments. append string|string[] Arguments to append to default arguments. Defaults core : disabled : false domain : ext : test sub : docker-devbox env : available : - prod - stage - ci - dev current : dev github_repository : gfi-centre-ouest/docker-devbox-ddb os : posix path : ddb_home : /home/devbox/.docker-devbox/ddb home : /home/devbox/.docker-devbox project_home : /home/devbox/projects/docker-devbox process : {} project : name : docker-devbox github_repository :","title":"core"},{"location":"features/core/#core","text":"Core feature contains some key configuration like domain name and current active environement. Those configuration settings may impact many other features indirectly. It also handle the two following basic commands : ddb features and ddb config . Check command page for details about those commands. Feature configuration (prefixed with core. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? domain.sub string ${project.name} The domain to use for the domain generation. This is the constant part of the domain, that should not vary between environment. domain.ext string test The extension to use for the domain. This is the last part, of your domain name, that may vary between environment. env.current string ${env.available}[-1] Current active environment. Default value is dev , or the last value of env.available . project.name string <Directory name of ${path.project_home}> The project name. This is used by many templates and to generate other default values like domain.sub . Advanced Property Type Description env.available string[] ['prod', 'stage', 'ci', 'dev'] List of available environments. You should any new custom environment to support here before trying to set env.current to this custom environment. required_version string Minimal required ddb version for the project to work properly. If required_version is greater than the currently running one, ddb will refuse to run until it's updated. path.ddb_home string ${env:HOME}/.docker-devbox/ddb The path where ddb is installed. path.home string ${env:HOME}/.docker-devbox The path where docker devbox is installed. path.project_home string The project directory. process Process [] List of process configurations. A process configuration allow to add custom arguments before and after a command executed internally (like git ). Internal Property Type Description os string posix The current operating system. github_repository string gfi-centre-ouest/docker-devbox-ddb List of process configurations. A process configuration allow to add flags before and after a command normaly runned by ddb (like git ). Process configuration (used in core.process ) Property Type Description bin string * The process path to override. prepend string|string[] Arguments to prepend to default arguments. append string|string[] Arguments to append to default arguments. Defaults core : disabled : false domain : ext : test sub : docker-devbox env : available : - prod - stage - ci - dev current : dev github_repository : gfi-centre-ouest/docker-devbox-ddb os : posix path : ddb_home : /home/devbox/.docker-devbox/ddb home : /home/devbox/.docker-devbox project_home : /home/devbox/projects/docker-devbox process : {} project : name : docker-devbox github_repository :","title":"Core"},{"location":"features/docker/","text":"Docker \u00b6 The docker feature brings docker and docker-compose integration. You may also check Jsonnet Feature , as using jsonnet docker specific library brings many Docker related features to ddb . Feature configuration (prefixed with symlinks. ) Property Type Description disabled boolean false Should this feature be disabled ? ip string IP Address of the docker engine interface string Network interface of the docker engine user.uid string The user UID to use inside a container when jsonnet User() function is used. user.gid string The user GID to use inside a container when jsonnet User() function is used. user.name string The host username that will get converted to UID to use inside a container when jsonnet User() function is used. user.group string The host groupname that will get converted to GID to use inside a container when jsonnet User() function is used. Defaults docker : interface : docker0 ip : 172.17.0.1 user : gid : 1000 uid : 1000 group : null name : null docker-compose configuration processing \u00b6 When a docker-compose.yml is found or generated from templates, the content is parsed. All labels prefixed ddb.emit. are processed and converted into event and event arguments. Creation of binaries Whether you use ddb.Binary() in jsonnet template or manually add labels to docker-compose.yml , they are converted into ddb configuration and shims are generated to run the declared binary as simple executable command, thanks to shell feature.","title":"docker"},{"location":"features/docker/#docker","text":"The docker feature brings docker and docker-compose integration. You may also check Jsonnet Feature , as using jsonnet docker specific library brings many Docker related features to ddb . Feature configuration (prefixed with symlinks. ) Property Type Description disabled boolean false Should this feature be disabled ? ip string IP Address of the docker engine interface string Network interface of the docker engine user.uid string The user UID to use inside a container when jsonnet User() function is used. user.gid string The user GID to use inside a container when jsonnet User() function is used. user.name string The host username that will get converted to UID to use inside a container when jsonnet User() function is used. user.group string The host groupname that will get converted to GID to use inside a container when jsonnet User() function is used. Defaults docker : interface : docker0 ip : 172.17.0.1 user : gid : 1000 uid : 1000 group : null name : null","title":"Docker"},{"location":"features/docker/#docker-compose-configuration-processing","text":"When a docker-compose.yml is found or generated from templates, the content is parsed. All labels prefixed ddb.emit. are processed and converted into event and event arguments. Creation of binaries Whether you use ddb.Binary() in jsonnet template or manually add labels to docker-compose.yml , they are converted into ddb configuration and shims are generated to run the declared binary as simple executable command, thanks to shell feature.","title":"docker-compose configuration processing"},{"location":"features/file/","text":"File \u00b6 As ddb work on files, there is a dedicated feature which soul purpose is to walk through files from the project directory and trigger events when files are found or removed. Feature configuration (prefixed with jinja. ) Property Type Description disabled boolean false Should this feature be disabled ? extensions string[] A list of glob of supported extension. includes string[] A list of glob of filepath to include. excludes string[] ['**/.git', '**/node_modules', '**/vendor', '**/target', '**/dist'] A list of glob of filepath to exclude. Defaults file : disabled : false excludes : - '**/.git' - '**/node_modules' - '**/vendor' - '**/target' - '**/dist' File walk and event triggering \u00b6 As previously said, the soul purpose of the feature is to walk through files from the project directory, recursively. It does not check folders and files set in file.excludes configuration. For instance, the directory node_modules which contains npm installed modules on the project is not process. By doing so, the feature is faster. When it checks for a folder, each file not excluded is put into a file:found event which will be caught by other features such as jsonnet or docker feature, and store those file in ddb cache. For next executions, using this cache, the feature will detect if a file have been deleted and raise a file.deleted . This event is used by the gitignore feature to remove the file from the gitignore if needed. The watch mode This feature is the one that will benefit the most from the watch mode described in the command section. It will then be kept active and check if a file is created, modified, moved or even deleted.","title":"file"},{"location":"features/file/#file","text":"As ddb work on files, there is a dedicated feature which soul purpose is to walk through files from the project directory and trigger events when files are found or removed. Feature configuration (prefixed with jinja. ) Property Type Description disabled boolean false Should this feature be disabled ? extensions string[] A list of glob of supported extension. includes string[] A list of glob of filepath to include. excludes string[] ['**/.git', '**/node_modules', '**/vendor', '**/target', '**/dist'] A list of glob of filepath to exclude. Defaults file : disabled : false excludes : - '**/.git' - '**/node_modules' - '**/vendor' - '**/target' - '**/dist'","title":"File"},{"location":"features/file/#file-walk-and-event-triggering","text":"As previously said, the soul purpose of the feature is to walk through files from the project directory, recursively. It does not check folders and files set in file.excludes configuration. For instance, the directory node_modules which contains npm installed modules on the project is not process. By doing so, the feature is faster. When it checks for a folder, each file not excluded is put into a file:found event which will be caught by other features such as jsonnet or docker feature, and store those file in ddb cache. For next executions, using this cache, the feature will detect if a file have been deleted and raise a file.deleted . This event is used by the gitignore feature to remove the file from the gitignore if needed. The watch mode This feature is the one that will benefit the most from the watch mode described in the command section. It will then be kept active and check if a file is created, modified, moved or even deleted.","title":"File walk and event triggering"},{"location":"features/fixuid/","text":"Fixuid \u00b6 One common pitfall when working with Docker is file permission management of mounted volumes. Those permission issues are related to the way docker works and cannot really be fixed once for all. To help developer fixing permission issues, fixuid is auto-configured by ddb when a fixuid.yml file is available in docker build context. Feature configuration (prefixed with fixuid. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? Internal Property Type Description url string https://github.com/boxboat/fixuid/releases/download/v0.5/fixuid-0.5-linux-amd64.tar.gz URL to download the fixuid distribution binary. Defaults fixuid : disabled : false Automatic configuration \u00b6 In order to benefit from this feature, few steps are require. First, you need to create the fixuid.yml configuration file next to Dockerfile.jinja . For this feature to work properly, you must use a template feature for your Dockerfile, like Jinja . In this fixuid.yml configuration file, you have to define three settings: user : the user inside the container which is allowed to run command and access files. group : the group inside the container which is allowed to run command and access files. paths : the list of paths inside the container where permissions will be fixed. Volumes mount point must also be listed. For more details on this configuration, please refer to the fixuid documentation . Example : A posgresql fixuid configuration user : postgres group : postgres paths : - / - /var/lib/postgresql/data Then, in your docker-compose.yml.jsonnet , you should use ddb.User() in order to map your local user to the container. Finally, run the ddb configure command to generate Dockerfile . Instructions related to fixuid should have been generated. The entrypoint is changed to run fixuid before the default entrypoint. Example with PostgreSQL FROM postgres # Mount this volume to help loading/exporting dumps RUN mkdir /workdir VOLUME /workdir USER postgres Generates the following when fixuid.yml file is available in the Dockerfile directory. FROM postgres # Mount this volume to help loading/exporting dumps RUN mkdir /workdir VOLUME /workdir USER postgres ADD fixuid.tar.gz /usr/local/bin RUN chown root:root /usr/local/bin/fixuid && chmod 4755 /usr/local/bin/fixuid && mkdir -p /etc/fixuid COPY fixuid.yml /etc/fixuid/config.yml USER postgres ENTRYPOINT [ \"fixuid\" , \"-q\" , \"docker-entrypoint.sh\" ] CMD [ \"postgres\" ] With this configuration, you should be able to generate a dump from the container as your own user instead of root. Why should I use .jinja extension for my Dockerfile ? You should always use .jinja when declaring a Dockerfile to benefits of fixuid feature. Using fixuid feature updates Dockerfile to add the instructions needed for fixuid. That's why you should use a template source instead, for this to be generated again on each ddb configure command. The Dockerfile will be automatically ignored by git thanks to gitignore feature. If you are not using jsonnet for docker-compose.yml In docker-compose.yml , you will need to add the configuration user to the service using this docker container. You will need to set it manually with the uid and gid of the host user which will run the container and execute commands. As many other ddb features are available through jsonnet feature , you should really consider using it. This will help you to build simpler, shorter and smarter docker-compose configuration. Disable or customize fixuid automatic configuration You may need to disable fixuid automatic configuration. Use the following comments in your Dockerfile.jinja. # fixuid-disable : Disable both download of fixuid.tar.gz and whole auto-configuration. Use this to disable fixuid totally for this Dockerfile. # fixuid-manual : Only keep download of fixuid.tar.gz. Use this to configure fixuid totally manually in the Dockerfile. # fixuid-manual-install : Keep download of fixuid.tar.gz and auto-configuration of ENTRYPOINT. You still have to install fixuid manually in the Dockerfile. # fixuid-manual-entrypoint : Keep download of fixuid.tar.gz and installation of required files. You still have to invoke fixuid manually in the Entrypoint.","title":"fixuid"},{"location":"features/fixuid/#fixuid","text":"One common pitfall when working with Docker is file permission management of mounted volumes. Those permission issues are related to the way docker works and cannot really be fixed once for all. To help developer fixing permission issues, fixuid is auto-configured by ddb when a fixuid.yml file is available in docker build context. Feature configuration (prefixed with fixuid. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? Internal Property Type Description url string https://github.com/boxboat/fixuid/releases/download/v0.5/fixuid-0.5-linux-amd64.tar.gz URL to download the fixuid distribution binary. Defaults fixuid : disabled : false","title":"Fixuid"},{"location":"features/fixuid/#automatic-configuration","text":"In order to benefit from this feature, few steps are require. First, you need to create the fixuid.yml configuration file next to Dockerfile.jinja . For this feature to work properly, you must use a template feature for your Dockerfile, like Jinja . In this fixuid.yml configuration file, you have to define three settings: user : the user inside the container which is allowed to run command and access files. group : the group inside the container which is allowed to run command and access files. paths : the list of paths inside the container where permissions will be fixed. Volumes mount point must also be listed. For more details on this configuration, please refer to the fixuid documentation . Example : A posgresql fixuid configuration user : postgres group : postgres paths : - / - /var/lib/postgresql/data Then, in your docker-compose.yml.jsonnet , you should use ddb.User() in order to map your local user to the container. Finally, run the ddb configure command to generate Dockerfile . Instructions related to fixuid should have been generated. The entrypoint is changed to run fixuid before the default entrypoint. Example with PostgreSQL FROM postgres # Mount this volume to help loading/exporting dumps RUN mkdir /workdir VOLUME /workdir USER postgres Generates the following when fixuid.yml file is available in the Dockerfile directory. FROM postgres # Mount this volume to help loading/exporting dumps RUN mkdir /workdir VOLUME /workdir USER postgres ADD fixuid.tar.gz /usr/local/bin RUN chown root:root /usr/local/bin/fixuid && chmod 4755 /usr/local/bin/fixuid && mkdir -p /etc/fixuid COPY fixuid.yml /etc/fixuid/config.yml USER postgres ENTRYPOINT [ \"fixuid\" , \"-q\" , \"docker-entrypoint.sh\" ] CMD [ \"postgres\" ] With this configuration, you should be able to generate a dump from the container as your own user instead of root. Why should I use .jinja extension for my Dockerfile ? You should always use .jinja when declaring a Dockerfile to benefits of fixuid feature. Using fixuid feature updates Dockerfile to add the instructions needed for fixuid. That's why you should use a template source instead, for this to be generated again on each ddb configure command. The Dockerfile will be automatically ignored by git thanks to gitignore feature. If you are not using jsonnet for docker-compose.yml In docker-compose.yml , you will need to add the configuration user to the service using this docker container. You will need to set it manually with the uid and gid of the host user which will run the container and execute commands. As many other ddb features are available through jsonnet feature , you should really consider using it. This will help you to build simpler, shorter and smarter docker-compose configuration. Disable or customize fixuid automatic configuration You may need to disable fixuid automatic configuration. Use the following comments in your Dockerfile.jinja. # fixuid-disable : Disable both download of fixuid.tar.gz and whole auto-configuration. Use this to disable fixuid totally for this Dockerfile. # fixuid-manual : Only keep download of fixuid.tar.gz. Use this to configure fixuid totally manually in the Dockerfile. # fixuid-manual-install : Keep download of fixuid.tar.gz and auto-configuration of ENTRYPOINT. You still have to install fixuid manually in the Dockerfile. # fixuid-manual-entrypoint : Keep download of fixuid.tar.gz and installation of required files. You still have to invoke fixuid manually in the Entrypoint.","title":"Automatic configuration"},{"location":"features/git/","text":"Git \u00b6 The Git feature provides automation relatives to the git configuration of the project. Feature configuration (prefixed with git. ) Property Type Description disabled boolean false Should this feature be disabled ? fix_files_permissions boolean true Should file permissions be fixed from git index (executable flags) ? Defaults git : disabled : false Configuration git : disabled : false fix_files_permissions : true Fix files permissions \u00b6 When you clone or update a repository, it may contain executable files. If you are working on windows and using some synchronisation too to synchronize those files, executable flags may be lost. With Git, you can store the executable flag into the repository. To do so, you can execute the following command : git update-index --chmod=+x <your_file> But this will not update the flag on the system automatically. Unless git.fix_files_permissions is set to false in ddb configuration, files marked as executable in git repository have their permissions fixed on ddb configure command.","title":"git"},{"location":"features/git/#git","text":"The Git feature provides automation relatives to the git configuration of the project. Feature configuration (prefixed with git. ) Property Type Description disabled boolean false Should this feature be disabled ? fix_files_permissions boolean true Should file permissions be fixed from git index (executable flags) ? Defaults git : disabled : false Configuration git : disabled : false fix_files_permissions : true","title":"Git"},{"location":"features/git/#fix-files-permissions","text":"When you clone or update a repository, it may contain executable files. If you are working on windows and using some synchronisation too to synchronize those files, executable flags may be lost. With Git, you can store the executable flag into the repository. To do so, you can execute the following command : git update-index --chmod=+x <your_file> But this will not update the flag on the system automatically. Unless git.fix_files_permissions is set to false in ddb configuration, files marked as executable in git repository have their permissions fixed on ddb configure command.","title":"Fix files permissions"},{"location":"features/gitignore/","text":"Gitignore \u00b6 The Gitignore feature provides automation to the management of .gitignore files of the project. Feature configuration (prefixed with gitignore. ) Property Type Description disabled boolean false Should this feature be disabled ? enforce string[] ['*ddb.local.*'] List of file globs to force into .gitignore . Defaults gitignore : disabled : false Automatic management of gitignore \u00b6 ddb generates a bunch of files inside your project sources: target files from templates, binary shims, ... As those files may depends on your environment configuration, they should not be added to the git repository and must be added to .gitignore , for them to be generated on each environment without spoiling git repository. Because adding them manually to .gitignore is a chore, ddb automatically adds all generated files to the nearest .gitignore file, from top to bottom of filesystem folder hierarchy. The other way round, if a generated file source is removed, the target file will also be removed from .gitignore file. Finally, using the enforce configuration, you can force files to be added to the gitignore, even if it is not a file managed by ddb.","title":"gitignore"},{"location":"features/gitignore/#gitignore","text":"The Gitignore feature provides automation to the management of .gitignore files of the project. Feature configuration (prefixed with gitignore. ) Property Type Description disabled boolean false Should this feature be disabled ? enforce string[] ['*ddb.local.*'] List of file globs to force into .gitignore . Defaults gitignore : disabled : false","title":"Gitignore"},{"location":"features/gitignore/#automatic-management-of-gitignore","text":"ddb generates a bunch of files inside your project sources: target files from templates, binary shims, ... As those files may depends on your environment configuration, they should not be added to the git repository and must be added to .gitignore , for them to be generated on each environment without spoiling git repository. Because adding them manually to .gitignore is a chore, ddb automatically adds all generated files to the nearest .gitignore file, from top to bottom of filesystem folder hierarchy. The other way round, if a generated file source is removed, the target file will also be removed from .gitignore file. Finally, using the enforce configuration, you can force files to be added to the gitignore, even if it is not a file managed by ddb.","title":"Automatic management of gitignore"},{"location":"features/jinja/","text":"Jinja \u00b6 Jinja is template library included in ddb . It is used to generate files using ddb configuration. Feature configuration (prefixed with jinja. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? suffixes string[] ['.jinja'] A list of filename suffix to include. extensions string[] ['.*', ''] A list of glob of supported extension. excludes string[] [] A list of glob of filepath to exclude. options dict[string, object] Additional options to pass to Jinja Environment . Advanced Property Type Description includes string[] ['*.jinja{.*,}'] A list of glob of filepath to include. It is automatically generated from suffixes and extensions . Defaults jinja : disabled : false extensions : - .* - '' includes : - '*.jinja{.*,}' suffixes : - .jinja Jinja template processing \u00b6 When running the ddb configuration command, ddb will look for files matching the list of includes configuration and not in the excludes list. For each file, it will be processed into his final form. In those templates, you can retrieve ddb configuration values simply using the full name of the variable. But ... how do I retrieve the full name of a variable ? Well, as you might already know, you can execute ddb config in order to check the configuration used in ddb. But if you append --variables to this command, you will have the variable name to include in your template ! dotenv .env configuration file generation You want to generate a .env file based on your current ddb configuration. You can create a .env.jinja and replace the parts you need to fill with those variables: APP_ENV = dev APP_SECRET = 4271e37e11180de028f11a132b453fb6 CORS_ALLOW_ORIGIN = ^https?:// {{ core.domain.sub }} \\. {{ core.domain.ext }} $ DATABASE_URL = mysql://ddb:ddb@db:3306/ddb MAILER_URL = smtp://mail As you can see, we have configured CORS_ALLOW_ORIGIN using ddb configuration variables. Now, we can run ddb configure . After the file have been processed, .env file is generated next to .env.jinja template file. With core.domain.sub set to ddb and core.domain.ext set to 'test', you will get the following content : APP_ENV = dev APP_SECRET = 4271e37e11180de028f11a132b453fb6 CORS_ALLOW_ORIGIN = ^https?://ddb \\. test$ DATABASE_URL = mysql://ddb:ddb@db:3306/ddb MAILER_URL = smtp://mail This way, you don't need to manually update .env file manually and can keep the whole project configuration centralized.","title":"jinja"},{"location":"features/jinja/#jinja","text":"Jinja is template library included in ddb . It is used to generate files using ddb configuration. Feature configuration (prefixed with jinja. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? suffixes string[] ['.jinja'] A list of filename suffix to include. extensions string[] ['.*', ''] A list of glob of supported extension. excludes string[] [] A list of glob of filepath to exclude. options dict[string, object] Additional options to pass to Jinja Environment . Advanced Property Type Description includes string[] ['*.jinja{.*,}'] A list of glob of filepath to include. It is automatically generated from suffixes and extensions . Defaults jinja : disabled : false extensions : - .* - '' includes : - '*.jinja{.*,}' suffixes : - .jinja","title":"Jinja"},{"location":"features/jinja/#jinja-template-processing","text":"When running the ddb configuration command, ddb will look for files matching the list of includes configuration and not in the excludes list. For each file, it will be processed into his final form. In those templates, you can retrieve ddb configuration values simply using the full name of the variable. But ... how do I retrieve the full name of a variable ? Well, as you might already know, you can execute ddb config in order to check the configuration used in ddb. But if you append --variables to this command, you will have the variable name to include in your template ! dotenv .env configuration file generation You want to generate a .env file based on your current ddb configuration. You can create a .env.jinja and replace the parts you need to fill with those variables: APP_ENV = dev APP_SECRET = 4271e37e11180de028f11a132b453fb6 CORS_ALLOW_ORIGIN = ^https?:// {{ core.domain.sub }} \\. {{ core.domain.ext }} $ DATABASE_URL = mysql://ddb:ddb@db:3306/ddb MAILER_URL = smtp://mail As you can see, we have configured CORS_ALLOW_ORIGIN using ddb configuration variables. Now, we can run ddb configure . After the file have been processed, .env file is generated next to .env.jinja template file. With core.domain.sub set to ddb and core.domain.ext set to 'test', you will get the following content : APP_ENV = dev APP_SECRET = 4271e37e11180de028f11a132b453fb6 CORS_ALLOW_ORIGIN = ^https?://ddb \\. test$ DATABASE_URL = mysql://ddb:ddb@db:3306/ddb MAILER_URL = smtp://mail This way, you don't need to manually update .env file manually and can keep the whole project configuration centralized.","title":"Jinja template processing"},{"location":"features/jsonnet/","text":"Jsonnet \u00b6 Jsonnet is embedded into ddb. You only have to use the right file extension for ddb to process it through the appropriate template engine. Mainly used to generate docker-compose.yml configuration, you can still use it for any other json or yaml based templating purpose. When a jsonnet template is processed, ddb use it's configuration as context, so you can use them as you need using jsonnet standard library function. s t d.ex t Var( \"<name of the configuration variable>\" ) Run ddb configure to evaluate templates and generate target files. Feature configuration (prefixed with jsonnet. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? suffixes string[] ['.jsonnet'] A list of filename suffix to include. extensions string[] ['.*', ''] A list of glob of supported extension. excludes string[] [] A list of glob of filepath to exclude. docker Docker Docker related configuration. Advanced Property Type Description includes string[] ['*.jsonnet{.*,}'] A list of glob of filepath to include. It is automatically generated from suffixes and extensions . Docker configuration (prefixed with jsonnet.docker. ) Simple Property Type Description compose Compose docker-compose defaults. networks Networks ddb.Networks() defaults. build Build ddb.Build() defaults. service Service ddb.Service() defaults. expose Expose ddb.Expose() defaults. registry Registry Image registry settings. user User ddb.User() defaults. binary Binary ddb.Binary() defaults. virtualhost VirtualHost ddb.VirtualHost() defaults. xdebug XDebug ddb.XDebug() defaults. Advanced Property Type Description path_mapping Dict[str, str] Path mappings to apply on declared volume sources. Docker Networks configuration (prefixed with jsonnet.docker.networks ) Simple Property Type Description names Dict[str, str] Additional external networks Docker Build configuration (prefixed with jsonnet.docker.build. ) Simple Property Type Description cache_from_image boolean False Add cache_from_image to configuration. context.base_directory string .docker Base directory for build context. context.use_project_home boolean False Use project home directory as build context. image_tag_from boolean string False Advanced Property Type Description image_tag string Image tag value. Docker Service configuration (prefixed with jsonnet.docker.service. ) Simple Property Type Description restart string unless-stopped no The restart policy to use for all services. Can be no , always , on-failure or unless-stopped . Default value is unless-stopped , unless core.env.current is set to dev then it's set to no . Advanced Property Type Description init string Docker Expose configuration (prefixed with jsonnet.docker.expose. ) Property Type Description disabled boolean False Should ddb.Expose() perform nothing ? port_prefix integer <based on core.project.name> Port prefix. Docker Registry configuration (prefixed with jsonnet.docker.registry. ) Property Type Description name string Registry name. repository string Registry repository. Docker User configuration (prefixed with jsonnet.docker.user. ) Simple Property Type Description uid string The user UID to use inside a container. gid string The user GID to use inside a container. name string The host username that will get converted to UID. group string The host groupname that will get converted to GID. Internal | name_to_uid | Dict[str, integer] | Mapping of user names to uid. | | group_to_gid | Dict[str, integer] | Mapping of group names to gid. | Docker Binary configuration (prefixed with jsonnet.docker.binary. ) Property Type Description disabled boolean False Should binary generation be disabled ? Docker VirtualHost configuration (prefixed with jsonnet.docker.virtualhost. ) Simple Property Type Description https string Should services be available as HTTPS ? If unset, it is available as both HTTP and HTTPS. redirect_to_https string Should services redirect to HTTPS when requested on HTTP ? redirect_to_path_prefix string Should services redirect to path_prefix when requested on root path ? certresolver string certresolver to use. letsencrypt is supported when using traefik reverse proxy. Internal | type | string traefik (when available) | Type of reverse proxy to use. | | network_id | string reverse-proxy | Network id used. | Docker XDebug configuration (prefixed with jsonnet.docker.xdebug. ) Simple Property Type Description disabled boolean false Should debug features be generated in docker-compose.yml by jsonnet feature ? host string ${jsonnet} The host to connect back for debug features. version string XDebug version to configure ( 2 or 3 ). If unset, both XDebug 2 and 3 configurations will generated in a merged object. session string ${core.project.name} XDebug session (v3) and/or idekey/serverName (v2) to configure. You can set this value explicitly to set env vars XDEBUG_SESSION (v3) and/or PHP_IDE_CONFIG / XDEBUG_CONFIG (v2). mode string debug XDebug mode (v3 only). Docker-compose jsonnet library \u00b6 Jsonnet feature provides a library with handful functions to help generate docker-compose.yml . In order to make those available, you need to import the library: local ddb = import 'ddb.docker.libjsonnet'; ddb.Compose() \u00b6 This function defines the main entrypoint to generate a docker-compose configuration. Parameters Property Type Description config object Docker compose configuration networks_names dict[str, str] ${jsonnet.docker.networks.names} Network id to name mapping version string ${jsonnet.docker.compose.version} docker-compose.yml file version. Example ddb.Compose() without any service configuration will produce version : '3.7' networks : reverse-proxy : external : true name : reverse-proxy Example ddb.Compose( { services : { db : { image : \"postgres\" } } } ) with a configuration with produce version : '3.7' networks : reverse-proxy : external : true name : reverse-proxy services : db : image : \"postgres\" ddb.Build() \u00b6 This function generates a service configuration from a Dockerfile available in .docker/<service> directory. If a docker registry is configured inside docker feature, image configuration will also be generated from the service name. Parameters Property Type Description name string Generate a build configuration based on given name. image string <name> The name of the image. If registry_name and/or registry_name settings are defined, it will generate the full image name. cache_from_image boolean ${jsonnet.docker.build.cache_from_image} If set to true and docker registry is defined, it will generate the build.cache_from configuration uri. context_base_directory boolean ${jsonnet.docker.build.context.base_directory} Build context base directory. context_use_project_home boolean ${jsonnet.docker.build.context.use_project_home} Use project home directory as context. restart string ${jsonnet.docker.service.restart} Service restart policy. init boolean ${jsonnet.docker.service.init} Service init. registry_name string ${jsonnet.docker.registry.name} Name of the docker image registry. registry_repository string ${jsonnet.docker.registry.repository} Repository in the docker image registry. image_tag boolean string ${jsonnet.docker.build.image_tag} Example with a registry defined if ddb.yml contains the following jsonnet: docker: registry: name: docker.io repository: project build: image_tag_from: True ddb.Build( \"db\" ) will produce build : context : .docker/db cache_from : docker.io/project/db:master image : docker.io/project/db:master ddb.Image() \u00b6 This function generates a service configuration based on an external image. It will also add the init to true and restart configurations for the service. The restart configuration will be set with the jsonnet.docker.service.restart ddb configuration. Parameters Property Type Description image string * The name of the image to use. restart string ${jsonnet.docker.service.restart} Service restart policy. init boolean ${jsonnet.docker.service.init} Service init. Example ddb.Image( \"nginx:latest\" ) will produce image : nginx:latest restart : no init : true ddb.Expose() \u00b6 This function generates an exposed port inside a service. It use jsonnet.docker.expose.port_prefix to generate a fixed mapped port in order to avoid port collisions between projects. Parameters Property Type Description container_port string|integer * Container port number to expose. host_port_suffix string End of the mapped port on host. from 1 to 99 . If null , use last 2 digits of container_port value. protocol string The protocol to use. Can be null , tcp or udp . port_prefix string|integer ${jsonnet.docker.expose.port_prefix} Port prefix. Example ddb.Expose( 21 ) + ddb.Expose( 22 , null , \"udp\" ) + ddb.Expose( 23 , 99 , \"tcp\" ) will produce ports : - '14721:21' - '14722:22/udp' - '14799:23/tcp' 147 is jsonnet.docker.expose.port_prefix configuration value. ddb.User() \u00b6 This function generates the user configuration for a Service. In ddb, it is mainly use for fixuid automatic integration Parameters Property Type Description uid string ${jsonnet.docker.user.uid} ${~jsonnet.docker.user.name} UID of user running the container. gid string ${jsonnet.docker.user.gid} ${~jsonnet.docker.user.group} GID of user running the container. Example ddb.User() will produce user : 1000:1000 userNameToUid and groupNameToGid to retrieve host uid/gid from names ddb.User(gid=ddb.groupNameToGid( \"docker\" )) will produce user : 1000:998 when getent group docker returns docker:x:998: on the host. It can be used when you need the container to access the docker socket through a volume mount. ddb.VirtualHost() \u00b6 This function generates service configuration used for reverse-proxy auto-configuration. The output generated depends on the jsonnet.docker.virtualhost.type ddb configuration. Currently, only traefik is supported. If this configuration is anything else, there will be no output. Parameters Property Type Description port string * HTTP port inside the container. hostname string * Hostname that will be exposed. name string Unique name for this VirtualHost. network_id string ${jsonnet.docker.virtualhost.network_id} The reverse-proxy network id. certresolver string ${jsonnet.docker.virtualhost.certresolver} certresolver to use inside reverse proxy (traefik). letsencrypt is supported when using traefik feature. router_rule string https string ${jsonnet.docker.virtualhost.https} Should service be available as HTTPS ? If unset, it is available as both HTTP and HTTPS. redirect_to_https string ${jsonnet.docker.virtualhost.redirect_to_https} Should service redirect to HTTPS when requested on HTTP ? path_prefix string Path prefix of this virtualhost redirect_to_path_prefix boolean Redirect to configured path prefix Example with traefik as reverse proxy ddb.Vir tual Hos t ( \"80\" , \"your-project.test\" , \"app\" ) will produce labels : traefik.enable : 'true' traefik.http.routers.your-project-app-tls.rule : Host(`your-project.test`) traefik.http.routers.your-project-app-tls.service : your-project-app traefik.http.routers.your-project-app-tls.tls : 'true' traefik.http.routers.your-project-app.rule : Host(`your-project.test`) traefik.http.routers.your-project-app.service : your-project-app traefik.http.services.your-project-app.loadbalancer.server.port : '80' networks : - default - reverse-proxy ddb.Binary() \u00b6 Binary allow the creation of alias for command execution inside the service. Parameters Property Type Description name string * Binary name. This will be the command you type in shell (see shell feature). workdir string Default container directory to run the command. In most case, it should match the service workdir. args string <name> Command to execute inside the container. options string Options to add to the command. options_condition string Add a condition to be evaluated to make options optional. If condition is defined and evaluated, options are not added to the command. exe boolean false launch command with docker-compose exec instead of run condition string Add a condition for the command to be enabled. If condition is defined and evaluated to false, command won't be used by run feature. Example ddb.Bi nar y( \"npm\" , \"/app\" , \"npm\" , \"--label traefik.enable=false\" , ' \"serve\" n o t i n args') will produce labels : ddb.emit.docker:binary[npm](args) : npm ddb.emit.docker:binary[npm](name) : npm ddb.emit.docker:binary[npm](options) : --label traefik.enable=false ddb.emit.docker:binary[npm](options_condition) : '\"serve\" not in args' ddb.emit.docker:binary[npm](workdir) : /app Register the same command many times You may want is some projects to have many binaries defined for the same command, i.e many version of composer or npm . It is possible to implement a condition on each Binary in order to enable one binary in a project directory, and another one in another project directory. Each binary sharing the same name should be defined in distinct services though, as it doesn't make sense to define the same command on the same service. You can find more information in run feature: Register many binaries for the same command . ddb.XDebug() (PHP) \u00b6 This function generates environment configuration used for XDebug (PHP Debugger). If jsonnet.docker.xdebug.disabled is set to true , the function returns an empty object. It will use the following ddb configuration to generate appropriate environment : core.project.name : XDebug 2: set serverName and idekey XDebug 3: set XDEBUG_SESSION jsonnet.docker.xdebug.host : XDebug 2: set remote_host XDebug 3: set client_host Parameters Property Type Description version string ${jsonnet.docker.xdebug.version} XDebug version to configure ( 2 or 3 ). If unset, both XDebug 2 and 3 configurations will generated in a merged object. session string ${jsonnet.docker.xdebug.session} XDebug session (v3) and/or idekey/serverName (v2) to configure. You can set this value explicitly to set env vars XDEBUG_SESSION (v3) and/or PHP_IDE_CONFIG / XDEBUG_CONFIG (v2). mode string ${jsonnet.docker.xdebug.mode} XDebug mode (v3 only). Example ddb.Xdebug() will produce environment : PHP_IDE_CONFIG : serverName=project-name XDEBUG_CONFIG : remote_enable=on remote_autostart=off idekey=project-name remote_host=192.168.85.1 ddb.env.is() \u00b6 This function allows you to check if the given environment is the current one, i.e. is equals to core.env.current . It does not have any input parameter and returns boolean. This can be used to add environment condition to a service activation, a specific configuration,... Parameters Property Type Description env string Environment name to verify. Example With the following configuration : core: env: current: dev ddb.env.is(\"prod\") => false ddb.env.is(\"dev\") => true Advanced functions \u00b6 Those functions are for advanced configuration and should not be used in most common cases. Advanced ddb.ServiceName() \u00b6 This function generates the right service name for a service. The main purpose is to have more easy way to manage Labels for traefik and easily add a middleware to a specific service. It concatenates the given name with ${core.project.name} . Parameters Property Type Description name string * Name of the service. Example With a project named \"ddb\" ddb.ServiceName( \"test\" ) will return ```yaml ddb-test ddb.BinaryLabels() \u00b6 BinaryLabels is mostly the same as Binary but output configuration without the labels: part, so you can add it directly to your own labels block. Parameters Property Type Description name string * Binary name. This will be the command you type in shell (see shell feature). workdir string Default container directory to run the command. In most case, it should match the service workdir. args string <name> Command to execute inside the container. options string Options to add to the command. options_condition string Add a condition to be evaluated to make options optional. If condition is defined and evaluated, options are not added to the command. Example ddb.Bi nar yLabels( \"npm\" , \"/app\" , \"npm\" , \"--label traefik.enable=false\" , ' \"serve\" n o t i n args') will produce ddb.emit.docker:binary[npm](args) : npm ddb.emit.docker:binary[npm](name) : npm ddb.emit.docker:binary[npm](options) : --label traefik.enable=false ddb.emit.docker:binary[npm](options_condition) : '\"serve\" not in args' ddb.emit.docker:binary[npm](workdir) : /app ddb.BinaryOptions() \u00b6 BinaryOptions allow you to add options to service's binary previously declared. Parameters Property Type Description name string * Binary name. This will be the command you type in shell (see shell feature). options string Options to add to the command. options_condition string Add a condition to be evaluated to make options optional. If condition is defined and evaluated, options are not added to the command. Example ddb.Bi nar yOp t io ns ( \"npm\" , \"--label traefik.enable=false\" , ' \"serve\" n o t i n args') will produce labels : ddb.emit.docker:binary[npm](options) : --label traefik.enable=false ddb.emit.docker:binary[npm](options_condition) : '\"serve\" not in args' ddb.BinaryOptionsLabels() \u00b6 BinaryOptionsLabels is mostly the same as BinaryOptions but output configuration without the labels: part, so you can add it directly to your own labels block. Parameters Property Type Description name string * Binary name. This will be the command you type in shell (see shell feature). options string Options to add to the command. options_condition string Add a condition to be evaluated to make options optional. If condition is defined and evaluated, options are not added to the command. Example ddb.Bi nar yOp t io ns ( \"npm\" , \"--label traefik.enable=false\" , ' \"serve\" n o t i n args') will produce labels : ddb.emit.docker:binary[npm](options) : --label traefik.enable=false ddb.emit.docker:binary[npm](options_condition) : '\"serve\" not in args' ddb.path \u00b6 Path is easy access to ddb configuration paths core.path values. It is mostly used to add a folder as volume to service. ddb.env.index() \u00b6 This function allows you to get the index of the given environment in the list core.env.available . If there is no parameter given, it provides the index of the current one. This can be used to add environment condition to a service activation, a specific configuration,... Parameters Property Type Description env string Environment name to verify. Example With the following configuration : core: env: available: - prod - stage - ci - dev current: dev ddb.env.index() will return 3 ddb.env.index(\"ci\") will return 2 ddb.JoinObjectArray() \u00b6 This function allows the generation of an array of object programmatically and then merge them which is not possible otherwise natively with jsonnet. Parameters Property Type Description object_array dict[] the array of objects to merge. Example With the following jsonnet declaration : local sites = ['www', 'api']; { \"web\": ddb.Build(\"web\") + ddb.JoinObjectArray([ddb.VirtualHost(\"80\", std.join('.', [site, \"domain.tld\"]), site) for site in sites]) + { \"volumes\": [ ddb.path.project + \"/.docker/web/nginx.conf:/etc/nginx/conf.d/default.conf:rw\", ddb.path.project + \":/var/www/html:rw\" ] } } The result will be labels : traefik.enable : \"true\" traefik.http.routers.your-project-api-tls.rule : Host(`api.domain.tld`) traefik.http.routers.your-project-api-tls.service : your-project-api traefik.http.routers.your-project-api-tls.tls : \"true\" traefik.http.routers.your-project-api.rule : Host(`api.domain.tld`) traefik.http.routers.your-project-api.service : your-project-api traefik.http.services.your-project-api.loadbalancer.server.port : '80' traefik.http.routers.your-project-www-tls.rule : Host(`www.domain.tld`) traefik.http.routers.your-project-www-tls.service : your-project-www traefik.http.routers.your-project-www-tls.tls : \"true\" traefik.http.routers.your-project-www.rule : Host(`www.domain.tld`) traefik.http.routers.your-project-www.service : your-project-www traefik.http.services.your-project-www.loadbalancer.server.port : '80' ddb.path.mapPath() \u00b6 Get the mapped value of a given filepath according to mappings configured in jsonnet.docker.path_mapping . Parameters Property Type Description path string Source path.","title":"jsonnet"},{"location":"features/jsonnet/#jsonnet","text":"Jsonnet is embedded into ddb. You only have to use the right file extension for ddb to process it through the appropriate template engine. Mainly used to generate docker-compose.yml configuration, you can still use it for any other json or yaml based templating purpose. When a jsonnet template is processed, ddb use it's configuration as context, so you can use them as you need using jsonnet standard library function. s t d.ex t Var( \"<name of the configuration variable>\" ) Run ddb configure to evaluate templates and generate target files. Feature configuration (prefixed with jsonnet. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? suffixes string[] ['.jsonnet'] A list of filename suffix to include. extensions string[] ['.*', ''] A list of glob of supported extension. excludes string[] [] A list of glob of filepath to exclude. docker Docker Docker related configuration. Advanced Property Type Description includes string[] ['*.jsonnet{.*,}'] A list of glob of filepath to include. It is automatically generated from suffixes and extensions . Docker configuration (prefixed with jsonnet.docker. ) Simple Property Type Description compose Compose docker-compose defaults. networks Networks ddb.Networks() defaults. build Build ddb.Build() defaults. service Service ddb.Service() defaults. expose Expose ddb.Expose() defaults. registry Registry Image registry settings. user User ddb.User() defaults. binary Binary ddb.Binary() defaults. virtualhost VirtualHost ddb.VirtualHost() defaults. xdebug XDebug ddb.XDebug() defaults. Advanced Property Type Description path_mapping Dict[str, str] Path mappings to apply on declared volume sources. Docker Networks configuration (prefixed with jsonnet.docker.networks ) Simple Property Type Description names Dict[str, str] Additional external networks Docker Build configuration (prefixed with jsonnet.docker.build. ) Simple Property Type Description cache_from_image boolean False Add cache_from_image to configuration. context.base_directory string .docker Base directory for build context. context.use_project_home boolean False Use project home directory as build context. image_tag_from boolean string False Advanced Property Type Description image_tag string Image tag value. Docker Service configuration (prefixed with jsonnet.docker.service. ) Simple Property Type Description restart string unless-stopped no The restart policy to use for all services. Can be no , always , on-failure or unless-stopped . Default value is unless-stopped , unless core.env.current is set to dev then it's set to no . Advanced Property Type Description init string Docker Expose configuration (prefixed with jsonnet.docker.expose. ) Property Type Description disabled boolean False Should ddb.Expose() perform nothing ? port_prefix integer <based on core.project.name> Port prefix. Docker Registry configuration (prefixed with jsonnet.docker.registry. ) Property Type Description name string Registry name. repository string Registry repository. Docker User configuration (prefixed with jsonnet.docker.user. ) Simple Property Type Description uid string The user UID to use inside a container. gid string The user GID to use inside a container. name string The host username that will get converted to UID. group string The host groupname that will get converted to GID. Internal | name_to_uid | Dict[str, integer] | Mapping of user names to uid. | | group_to_gid | Dict[str, integer] | Mapping of group names to gid. | Docker Binary configuration (prefixed with jsonnet.docker.binary. ) Property Type Description disabled boolean False Should binary generation be disabled ? Docker VirtualHost configuration (prefixed with jsonnet.docker.virtualhost. ) Simple Property Type Description https string Should services be available as HTTPS ? If unset, it is available as both HTTP and HTTPS. redirect_to_https string Should services redirect to HTTPS when requested on HTTP ? redirect_to_path_prefix string Should services redirect to path_prefix when requested on root path ? certresolver string certresolver to use. letsencrypt is supported when using traefik reverse proxy. Internal | type | string traefik (when available) | Type of reverse proxy to use. | | network_id | string reverse-proxy | Network id used. | Docker XDebug configuration (prefixed with jsonnet.docker.xdebug. ) Simple Property Type Description disabled boolean false Should debug features be generated in docker-compose.yml by jsonnet feature ? host string ${jsonnet} The host to connect back for debug features. version string XDebug version to configure ( 2 or 3 ). If unset, both XDebug 2 and 3 configurations will generated in a merged object. session string ${core.project.name} XDebug session (v3) and/or idekey/serverName (v2) to configure. You can set this value explicitly to set env vars XDEBUG_SESSION (v3) and/or PHP_IDE_CONFIG / XDEBUG_CONFIG (v2). mode string debug XDebug mode (v3 only).","title":"Jsonnet"},{"location":"features/jsonnet/#docker-compose-jsonnet-library","text":"Jsonnet feature provides a library with handful functions to help generate docker-compose.yml . In order to make those available, you need to import the library: local ddb = import 'ddb.docker.libjsonnet';","title":"Docker-compose jsonnet library"},{"location":"features/jsonnet/#ddbcompose","text":"This function defines the main entrypoint to generate a docker-compose configuration. Parameters Property Type Description config object Docker compose configuration networks_names dict[str, str] ${jsonnet.docker.networks.names} Network id to name mapping version string ${jsonnet.docker.compose.version} docker-compose.yml file version. Example ddb.Compose() without any service configuration will produce version : '3.7' networks : reverse-proxy : external : true name : reverse-proxy Example ddb.Compose( { services : { db : { image : \"postgres\" } } } ) with a configuration with produce version : '3.7' networks : reverse-proxy : external : true name : reverse-proxy services : db : image : \"postgres\"","title":"ddb.Compose()"},{"location":"features/jsonnet/#ddbbuild","text":"This function generates a service configuration from a Dockerfile available in .docker/<service> directory. If a docker registry is configured inside docker feature, image configuration will also be generated from the service name. Parameters Property Type Description name string Generate a build configuration based on given name. image string <name> The name of the image. If registry_name and/or registry_name settings are defined, it will generate the full image name. cache_from_image boolean ${jsonnet.docker.build.cache_from_image} If set to true and docker registry is defined, it will generate the build.cache_from configuration uri. context_base_directory boolean ${jsonnet.docker.build.context.base_directory} Build context base directory. context_use_project_home boolean ${jsonnet.docker.build.context.use_project_home} Use project home directory as context. restart string ${jsonnet.docker.service.restart} Service restart policy. init boolean ${jsonnet.docker.service.init} Service init. registry_name string ${jsonnet.docker.registry.name} Name of the docker image registry. registry_repository string ${jsonnet.docker.registry.repository} Repository in the docker image registry. image_tag boolean string ${jsonnet.docker.build.image_tag} Example with a registry defined if ddb.yml contains the following jsonnet: docker: registry: name: docker.io repository: project build: image_tag_from: True ddb.Build( \"db\" ) will produce build : context : .docker/db cache_from : docker.io/project/db:master image : docker.io/project/db:master","title":"ddb.Build()"},{"location":"features/jsonnet/#ddbimage","text":"This function generates a service configuration based on an external image. It will also add the init to true and restart configurations for the service. The restart configuration will be set with the jsonnet.docker.service.restart ddb configuration. Parameters Property Type Description image string * The name of the image to use. restart string ${jsonnet.docker.service.restart} Service restart policy. init boolean ${jsonnet.docker.service.init} Service init. Example ddb.Image( \"nginx:latest\" ) will produce image : nginx:latest restart : no init : true","title":"ddb.Image()"},{"location":"features/jsonnet/#ddbexpose","text":"This function generates an exposed port inside a service. It use jsonnet.docker.expose.port_prefix to generate a fixed mapped port in order to avoid port collisions between projects. Parameters Property Type Description container_port string|integer * Container port number to expose. host_port_suffix string End of the mapped port on host. from 1 to 99 . If null , use last 2 digits of container_port value. protocol string The protocol to use. Can be null , tcp or udp . port_prefix string|integer ${jsonnet.docker.expose.port_prefix} Port prefix. Example ddb.Expose( 21 ) + ddb.Expose( 22 , null , \"udp\" ) + ddb.Expose( 23 , 99 , \"tcp\" ) will produce ports : - '14721:21' - '14722:22/udp' - '14799:23/tcp' 147 is jsonnet.docker.expose.port_prefix configuration value.","title":"ddb.Expose()"},{"location":"features/jsonnet/#ddbuser","text":"This function generates the user configuration for a Service. In ddb, it is mainly use for fixuid automatic integration Parameters Property Type Description uid string ${jsonnet.docker.user.uid} ${~jsonnet.docker.user.name} UID of user running the container. gid string ${jsonnet.docker.user.gid} ${~jsonnet.docker.user.group} GID of user running the container. Example ddb.User() will produce user : 1000:1000 userNameToUid and groupNameToGid to retrieve host uid/gid from names ddb.User(gid=ddb.groupNameToGid( \"docker\" )) will produce user : 1000:998 when getent group docker returns docker:x:998: on the host. It can be used when you need the container to access the docker socket through a volume mount.","title":"ddb.User()"},{"location":"features/jsonnet/#ddbvirtualhost","text":"This function generates service configuration used for reverse-proxy auto-configuration. The output generated depends on the jsonnet.docker.virtualhost.type ddb configuration. Currently, only traefik is supported. If this configuration is anything else, there will be no output. Parameters Property Type Description port string * HTTP port inside the container. hostname string * Hostname that will be exposed. name string Unique name for this VirtualHost. network_id string ${jsonnet.docker.virtualhost.network_id} The reverse-proxy network id. certresolver string ${jsonnet.docker.virtualhost.certresolver} certresolver to use inside reverse proxy (traefik). letsencrypt is supported when using traefik feature. router_rule string https string ${jsonnet.docker.virtualhost.https} Should service be available as HTTPS ? If unset, it is available as both HTTP and HTTPS. redirect_to_https string ${jsonnet.docker.virtualhost.redirect_to_https} Should service redirect to HTTPS when requested on HTTP ? path_prefix string Path prefix of this virtualhost redirect_to_path_prefix boolean Redirect to configured path prefix Example with traefik as reverse proxy ddb.Vir tual Hos t ( \"80\" , \"your-project.test\" , \"app\" ) will produce labels : traefik.enable : 'true' traefik.http.routers.your-project-app-tls.rule : Host(`your-project.test`) traefik.http.routers.your-project-app-tls.service : your-project-app traefik.http.routers.your-project-app-tls.tls : 'true' traefik.http.routers.your-project-app.rule : Host(`your-project.test`) traefik.http.routers.your-project-app.service : your-project-app traefik.http.services.your-project-app.loadbalancer.server.port : '80' networks : - default - reverse-proxy","title":"ddb.VirtualHost()"},{"location":"features/jsonnet/#ddbbinary","text":"Binary allow the creation of alias for command execution inside the service. Parameters Property Type Description name string * Binary name. This will be the command you type in shell (see shell feature). workdir string Default container directory to run the command. In most case, it should match the service workdir. args string <name> Command to execute inside the container. options string Options to add to the command. options_condition string Add a condition to be evaluated to make options optional. If condition is defined and evaluated, options are not added to the command. exe boolean false launch command with docker-compose exec instead of run condition string Add a condition for the command to be enabled. If condition is defined and evaluated to false, command won't be used by run feature. Example ddb.Bi nar y( \"npm\" , \"/app\" , \"npm\" , \"--label traefik.enable=false\" , ' \"serve\" n o t i n args') will produce labels : ddb.emit.docker:binary[npm](args) : npm ddb.emit.docker:binary[npm](name) : npm ddb.emit.docker:binary[npm](options) : --label traefik.enable=false ddb.emit.docker:binary[npm](options_condition) : '\"serve\" not in args' ddb.emit.docker:binary[npm](workdir) : /app Register the same command many times You may want is some projects to have many binaries defined for the same command, i.e many version of composer or npm . It is possible to implement a condition on each Binary in order to enable one binary in a project directory, and another one in another project directory. Each binary sharing the same name should be defined in distinct services though, as it doesn't make sense to define the same command on the same service. You can find more information in run feature: Register many binaries for the same command .","title":"ddb.Binary()"},{"location":"features/jsonnet/#ddbxdebug-php","text":"This function generates environment configuration used for XDebug (PHP Debugger). If jsonnet.docker.xdebug.disabled is set to true , the function returns an empty object. It will use the following ddb configuration to generate appropriate environment : core.project.name : XDebug 2: set serverName and idekey XDebug 3: set XDEBUG_SESSION jsonnet.docker.xdebug.host : XDebug 2: set remote_host XDebug 3: set client_host Parameters Property Type Description version string ${jsonnet.docker.xdebug.version} XDebug version to configure ( 2 or 3 ). If unset, both XDebug 2 and 3 configurations will generated in a merged object. session string ${jsonnet.docker.xdebug.session} XDebug session (v3) and/or idekey/serverName (v2) to configure. You can set this value explicitly to set env vars XDEBUG_SESSION (v3) and/or PHP_IDE_CONFIG / XDEBUG_CONFIG (v2). mode string ${jsonnet.docker.xdebug.mode} XDebug mode (v3 only). Example ddb.Xdebug() will produce environment : PHP_IDE_CONFIG : serverName=project-name XDEBUG_CONFIG : remote_enable=on remote_autostart=off idekey=project-name remote_host=192.168.85.1","title":"ddb.XDebug() (PHP)"},{"location":"features/jsonnet/#ddbenvis","text":"This function allows you to check if the given environment is the current one, i.e. is equals to core.env.current . It does not have any input parameter and returns boolean. This can be used to add environment condition to a service activation, a specific configuration,... Parameters Property Type Description env string Environment name to verify. Example With the following configuration : core: env: current: dev ddb.env.is(\"prod\") => false ddb.env.is(\"dev\") => true","title":"ddb.env.is()"},{"location":"features/jsonnet/#advanced-functions","text":"Those functions are for advanced configuration and should not be used in most common cases. Advanced","title":"Advanced functions"},{"location":"features/jsonnet/#ddbservicename","text":"This function generates the right service name for a service. The main purpose is to have more easy way to manage Labels for traefik and easily add a middleware to a specific service. It concatenates the given name with ${core.project.name} . Parameters Property Type Description name string * Name of the service. Example With a project named \"ddb\" ddb.ServiceName( \"test\" ) will return ```yaml ddb-test","title":"ddb.ServiceName()"},{"location":"features/jsonnet/#ddbbinarylabels","text":"BinaryLabels is mostly the same as Binary but output configuration without the labels: part, so you can add it directly to your own labels block. Parameters Property Type Description name string * Binary name. This will be the command you type in shell (see shell feature). workdir string Default container directory to run the command. In most case, it should match the service workdir. args string <name> Command to execute inside the container. options string Options to add to the command. options_condition string Add a condition to be evaluated to make options optional. If condition is defined and evaluated, options are not added to the command. Example ddb.Bi nar yLabels( \"npm\" , \"/app\" , \"npm\" , \"--label traefik.enable=false\" , ' \"serve\" n o t i n args') will produce ddb.emit.docker:binary[npm](args) : npm ddb.emit.docker:binary[npm](name) : npm ddb.emit.docker:binary[npm](options) : --label traefik.enable=false ddb.emit.docker:binary[npm](options_condition) : '\"serve\" not in args' ddb.emit.docker:binary[npm](workdir) : /app","title":"ddb.BinaryLabels()"},{"location":"features/jsonnet/#ddbbinaryoptions","text":"BinaryOptions allow you to add options to service's binary previously declared. Parameters Property Type Description name string * Binary name. This will be the command you type in shell (see shell feature). options string Options to add to the command. options_condition string Add a condition to be evaluated to make options optional. If condition is defined and evaluated, options are not added to the command. Example ddb.Bi nar yOp t io ns ( \"npm\" , \"--label traefik.enable=false\" , ' \"serve\" n o t i n args') will produce labels : ddb.emit.docker:binary[npm](options) : --label traefik.enable=false ddb.emit.docker:binary[npm](options_condition) : '\"serve\" not in args'","title":"ddb.BinaryOptions()"},{"location":"features/jsonnet/#ddbbinaryoptionslabels","text":"BinaryOptionsLabels is mostly the same as BinaryOptions but output configuration without the labels: part, so you can add it directly to your own labels block. Parameters Property Type Description name string * Binary name. This will be the command you type in shell (see shell feature). options string Options to add to the command. options_condition string Add a condition to be evaluated to make options optional. If condition is defined and evaluated, options are not added to the command. Example ddb.Bi nar yOp t io ns ( \"npm\" , \"--label traefik.enable=false\" , ' \"serve\" n o t i n args') will produce labels : ddb.emit.docker:binary[npm](options) : --label traefik.enable=false ddb.emit.docker:binary[npm](options_condition) : '\"serve\" not in args'","title":"ddb.BinaryOptionsLabels()"},{"location":"features/jsonnet/#ddbpath","text":"Path is easy access to ddb configuration paths core.path values. It is mostly used to add a folder as volume to service.","title":"ddb.path"},{"location":"features/jsonnet/#ddbenvindex","text":"This function allows you to get the index of the given environment in the list core.env.available . If there is no parameter given, it provides the index of the current one. This can be used to add environment condition to a service activation, a specific configuration,... Parameters Property Type Description env string Environment name to verify. Example With the following configuration : core: env: available: - prod - stage - ci - dev current: dev ddb.env.index() will return 3 ddb.env.index(\"ci\") will return 2","title":"ddb.env.index()"},{"location":"features/jsonnet/#ddbjoinobjectarray","text":"This function allows the generation of an array of object programmatically and then merge them which is not possible otherwise natively with jsonnet. Parameters Property Type Description object_array dict[] the array of objects to merge. Example With the following jsonnet declaration : local sites = ['www', 'api']; { \"web\": ddb.Build(\"web\") + ddb.JoinObjectArray([ddb.VirtualHost(\"80\", std.join('.', [site, \"domain.tld\"]), site) for site in sites]) + { \"volumes\": [ ddb.path.project + \"/.docker/web/nginx.conf:/etc/nginx/conf.d/default.conf:rw\", ddb.path.project + \":/var/www/html:rw\" ] } } The result will be labels : traefik.enable : \"true\" traefik.http.routers.your-project-api-tls.rule : Host(`api.domain.tld`) traefik.http.routers.your-project-api-tls.service : your-project-api traefik.http.routers.your-project-api-tls.tls : \"true\" traefik.http.routers.your-project-api.rule : Host(`api.domain.tld`) traefik.http.routers.your-project-api.service : your-project-api traefik.http.services.your-project-api.loadbalancer.server.port : '80' traefik.http.routers.your-project-www-tls.rule : Host(`www.domain.tld`) traefik.http.routers.your-project-www-tls.service : your-project-www traefik.http.routers.your-project-www-tls.tls : \"true\" traefik.http.routers.your-project-www.rule : Host(`www.domain.tld`) traefik.http.routers.your-project-www.service : your-project-www traefik.http.services.your-project-www.loadbalancer.server.port : '80'","title":"ddb.JoinObjectArray()"},{"location":"features/jsonnet/#ddbpathmappath","text":"Get the mapped value of a given filepath according to mappings configured in jsonnet.docker.path_mapping . Parameters Property Type Description path string Source path.","title":"ddb.path.mapPath()"},{"location":"features/permissions/","text":"Permissions \u00b6 Permissions can be a bit tedious to update. Plus, you might want to define the right permissions on a file once and for all. Well, this feature is made for you! Feature configuration (prefixed with permissions. ) Property Type Description disabled boolean false Should this feature be disabled ? specs dict[string, string] Key of the dict is a filepath glob matching the files to change, value is a chmod-like permission modifier like +x or 400 . Defaults permissions : disabled : false Add executable permission to each file in bin/ directory permissions : specs : \"bin/*\" : \"+x\" Permission update \u00b6 The permission management is performed for file:found and file:generated events. When one of those is raised, the system check if the file associated to the event is part of the specs configuration. If it is part of it, the feature will update the permissions of the file accordingly to the chmod modifier defined.","title":"permissions"},{"location":"features/permissions/#permissions","text":"Permissions can be a bit tedious to update. Plus, you might want to define the right permissions on a file once and for all. Well, this feature is made for you! Feature configuration (prefixed with permissions. ) Property Type Description disabled boolean false Should this feature be disabled ? specs dict[string, string] Key of the dict is a filepath glob matching the files to change, value is a chmod-like permission modifier like +x or 400 . Defaults permissions : disabled : false Add executable permission to each file in bin/ directory permissions : specs : \"bin/*\" : \"+x\"","title":"Permissions"},{"location":"features/permissions/#permission-update","text":"The permission management is performed for file:found and file:generated events. When one of those is raised, the system check if the file associated to the event is part of the specs configuration. If it is part of it, the feature will update the permissions of the file accordingly to the chmod modifier defined.","title":"Permission update"},{"location":"features/run/","text":"Run \u00b6 As ddb is a way to simplify the way to work with docker, this feature is a very useful one. If you are working with the jsonnet feature and the Binary function in your docker-compose.yml.jsonnet , run feature allow you to run commands in your docker container just as if it was a native system command. Feature configuration (prefixed with run. ) Property Type Description disabled boolean false Should this feature be disabled ? Defaults run : disabled : false Run command in container as if it was native \u00b6 As said in introduction, the purpose of the run feature is to allow you to run commands in your docker container just as if it was a native system command. In order to do so, the feature will look in the project configuration for the binary you are trying to run. Then, it will generate the right docker-compose command to be executed. But, as we are lazy, it was not enough. So, when you create a project the docker will trigger the shell one, which will generate a binary file for the current project. This file will simply execute the ddb run <binary_name> with the input argument and automatically execute the result. The combination of the previously named feature with the current one will grant you the capability to execute npm just as you would do with a system installation of it. How to run npm or composer command ? If you follow the Guide for PostgreSQL, Symfony, VueJS , you will be able to execute ddb run npm which will output the following command docker-compose run --rm --workdir=/app/. node npm . But, as explained, ddb will also create a binary shim for npm so you can run it like a native command, which is far more easy to use for everyone. Run $(ddb activate), and then npm is available right in your shell path. Register many binaries for the same command \u00b6 When a binary shim is invoked, it runs ddb run <command-name> under the hood. All declared binaries matching the command name with a condition defined will have their condition evaluated, and the first one returning True is used to generate the effective command output. If no condition match, the first remaining one is used. If no binary is finally found, the command is wrapped into equivalent of $(ddb deactivate) / $(ddb activate) instruction so it runs on the host system.","title":"run"},{"location":"features/run/#run","text":"As ddb is a way to simplify the way to work with docker, this feature is a very useful one. If you are working with the jsonnet feature and the Binary function in your docker-compose.yml.jsonnet , run feature allow you to run commands in your docker container just as if it was a native system command. Feature configuration (prefixed with run. ) Property Type Description disabled boolean false Should this feature be disabled ? Defaults run : disabled : false","title":"Run"},{"location":"features/run/#run-command-in-container-as-if-it-was-native","text":"As said in introduction, the purpose of the run feature is to allow you to run commands in your docker container just as if it was a native system command. In order to do so, the feature will look in the project configuration for the binary you are trying to run. Then, it will generate the right docker-compose command to be executed. But, as we are lazy, it was not enough. So, when you create a project the docker will trigger the shell one, which will generate a binary file for the current project. This file will simply execute the ddb run <binary_name> with the input argument and automatically execute the result. The combination of the previously named feature with the current one will grant you the capability to execute npm just as you would do with a system installation of it. How to run npm or composer command ? If you follow the Guide for PostgreSQL, Symfony, VueJS , you will be able to execute ddb run npm which will output the following command docker-compose run --rm --workdir=/app/. node npm . But, as explained, ddb will also create a binary shim for npm so you can run it like a native command, which is far more easy to use for everyone. Run $(ddb activate), and then npm is available right in your shell path.","title":"Run command in container as if it was native"},{"location":"features/run/#register-many-binaries-for-the-same-command","text":"When a binary shim is invoked, it runs ddb run <command-name> under the hood. All declared binaries matching the command name with a condition defined will have their condition evaluated, and the first one returning True is used to generate the effective command output. If no condition match, the first remaining one is used. If no binary is finally found, the command is wrapped into equivalent of $(ddb deactivate) / $(ddb activate) instruction so it runs on the host system.","title":"Register many binaries for the same command"},{"location":"features/shell/","text":"Shell \u00b6 The shell feature manages OS/Shell specific behaviors (Windows, Linux/Unix). For instance, generated binary shims are bash executables on Linux, but .bat files on Windows. Feature configuration (prefixed with symlinks. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? aliases dict[string, string] Allow the creation of aliases. global_aliases string[] Aliases matching those names are available globally instead of inside project only. Advanced Property Type Description envignore string[] [\"PYENV_*\", \"_\", \"PS1\", \"PS2\", \"PS3\", \"PS4\", \"PWD\"] When activating ddb for a project via $(ddb activate) , environment variables are saved before being updated. This list is those who will not be saved and updated by the command. path.directories string[] [\".bin\", \"bin\"] List of directories to add to PATH environment variable when running $(ddb activate) . The first one from this list is also used as root folder for binaries and aliases shims generation. path.prepend bollean true Should paths declared in path.directories be placed at the begging of PATH environment variable. If set to false , it will be added to the end. Internal Property Type Description shell string bash (linux) bash (macos) cmd (windows) Type of shell to work with. Currently, only bash and cmd (windows) are supported. Defaults shell : disabled : false envignore : - PYENV_* - _ - PS1 - PS2 - PS3 - PS4 path : directories : - .bin - bin prepend : true shell : bash Defaults shell : disabled : false envignore : - PYENV_* - _ - PS1 - PS2 - PS3 - PS4 - PWD path : directories : - .bin - bin prepend : true shell : bash Environment activation \u00b6 ddb is generating configurations for your project and brings docker container binaries right in your development environment. If you have registered binary inside docker-compose.yml.jsonnet , you can found binary shims inside .bin directory. It means that you need to be in the root folder of your project, or to add the full path to the executable file if you are not. The best solution is to update your PATH . ddb provides you a way to do it by running the $(ddb activate) command in your shell. This command will generate environment variables updates commands and execute them, including PATH update for easy to access stored in the .bin folder of your project from anywhere. Changes to environment are local and not persistent The modification of the shell environment is local to your current shell session. It means that if you open a new one, the environment will be the same as before run the $(ddb activate) command. But what happen when switching to another project ? The environment variables will still be configured for the project you ran the command for. So before leaving his directory, you will need to run $(ddb deactivate) command. It will unload the project specific environment variable configuration and restore it to the initial state. Now you can move to the new project directory and run $(ddb activate) once again, and so on. If you are as lazy as we are ... In order to automate this process, check SmartCD Feature . It will run those commands when entering and leaving the project directory containing the ddb.yml file. Docker binary shims \u00b6 If you are using Jsonnet templates with ddb.Binary() function, this means that you want to have simple access to command execution inside docker containers. As the generation of the shims depends on your specific shell (cmd.exe, bash, ...), it is handled by the shell feature. Each declared binary inside .jsonnet file generates a shim inside the .bin project directory (directory configured in shell.path.directories[0] ), and available in your shell after running $(ddb activate) . Access to PostgreSQL commands the native way If you have a PostgreSQL container, you may need to run commands like ... - psql - PostgreSQL client binary. - pg_dump - PostgreSQL command line tool to export data to a file. Instead of writing a long and impossible to remember docker commmand, you should declare them using ddb.Binary() function in docker-compose.yml.jsonnet file. local ddb = impor t 'ddb.docker.libjso nnet '; ddb.Compose( { services : { db : ddb.Image( \"postgres\" ) + ddb.Bi nar y( \"psql\" , \"/project\" , \"psql --dbname=postgresql://postgres:ddb@db/postgres\" ) + ddb.Bi nar y( \"pg_dump\" , \"/project\" , \"pg_dump --dbname=postgresql://postgres:ddb@db/postgres\" ) + { e n viro n me nt + : { POSTGRES_PASSWORD : \"ddb\" }, volumes+ : [ 'db - da ta : /var/lib/pos t gresql/da ta ' , ddb.pa t h.projec t + ' : /projec t ' ] } } } ) Then, run ddb configure , which will generate executable shim file in the .bin folder. Finaly, run $(ddb activate) to update your PATH and bring those commands to your local environment. Now, psql and pg_dump are available as if they were native commands. Aliases Management \u00b6 On your environment, aliases can be manually created in order to save time on repetitive commands execution or on long instructions. Some of those aliases are really useful only in a specific project context. The shell feature allows you to create your own aliases which will be generated the same way as docker binaries are. In order to declare them, you need to update the ddb configuration with the list of aliases : shell : aliases : myAlias : theLongCommandToExecute Make a composer dependency available globally When you are working on PHP/Composer project, some commands are available inside vendor path. For drupal developers, Drush commands is available through vendor/drush/drush/drush binary. Instead of writing the full path of this binary each time, you can declare an alias in ddb configuration. shell : aliases : drush : vendor/drush/drush/drush A binary shim will be created and added to the PATH thanks to ddb so you will able to use it from you project root folder: drush cr","title":"shell"},{"location":"features/shell/#shell","text":"The shell feature manages OS/Shell specific behaviors (Windows, Linux/Unix). For instance, generated binary shims are bash executables on Linux, but .bat files on Windows. Feature configuration (prefixed with symlinks. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? aliases dict[string, string] Allow the creation of aliases. global_aliases string[] Aliases matching those names are available globally instead of inside project only. Advanced Property Type Description envignore string[] [\"PYENV_*\", \"_\", \"PS1\", \"PS2\", \"PS3\", \"PS4\", \"PWD\"] When activating ddb for a project via $(ddb activate) , environment variables are saved before being updated. This list is those who will not be saved and updated by the command. path.directories string[] [\".bin\", \"bin\"] List of directories to add to PATH environment variable when running $(ddb activate) . The first one from this list is also used as root folder for binaries and aliases shims generation. path.prepend bollean true Should paths declared in path.directories be placed at the begging of PATH environment variable. If set to false , it will be added to the end. Internal Property Type Description shell string bash (linux) bash (macos) cmd (windows) Type of shell to work with. Currently, only bash and cmd (windows) are supported. Defaults shell : disabled : false envignore : - PYENV_* - _ - PS1 - PS2 - PS3 - PS4 path : directories : - .bin - bin prepend : true shell : bash Defaults shell : disabled : false envignore : - PYENV_* - _ - PS1 - PS2 - PS3 - PS4 - PWD path : directories : - .bin - bin prepend : true shell : bash","title":"Shell"},{"location":"features/shell/#environment-activation","text":"ddb is generating configurations for your project and brings docker container binaries right in your development environment. If you have registered binary inside docker-compose.yml.jsonnet , you can found binary shims inside .bin directory. It means that you need to be in the root folder of your project, or to add the full path to the executable file if you are not. The best solution is to update your PATH . ddb provides you a way to do it by running the $(ddb activate) command in your shell. This command will generate environment variables updates commands and execute them, including PATH update for easy to access stored in the .bin folder of your project from anywhere. Changes to environment are local and not persistent The modification of the shell environment is local to your current shell session. It means that if you open a new one, the environment will be the same as before run the $(ddb activate) command. But what happen when switching to another project ? The environment variables will still be configured for the project you ran the command for. So before leaving his directory, you will need to run $(ddb deactivate) command. It will unload the project specific environment variable configuration and restore it to the initial state. Now you can move to the new project directory and run $(ddb activate) once again, and so on. If you are as lazy as we are ... In order to automate this process, check SmartCD Feature . It will run those commands when entering and leaving the project directory containing the ddb.yml file.","title":"Environment activation"},{"location":"features/shell/#docker-binary-shims","text":"If you are using Jsonnet templates with ddb.Binary() function, this means that you want to have simple access to command execution inside docker containers. As the generation of the shims depends on your specific shell (cmd.exe, bash, ...), it is handled by the shell feature. Each declared binary inside .jsonnet file generates a shim inside the .bin project directory (directory configured in shell.path.directories[0] ), and available in your shell after running $(ddb activate) . Access to PostgreSQL commands the native way If you have a PostgreSQL container, you may need to run commands like ... - psql - PostgreSQL client binary. - pg_dump - PostgreSQL command line tool to export data to a file. Instead of writing a long and impossible to remember docker commmand, you should declare them using ddb.Binary() function in docker-compose.yml.jsonnet file. local ddb = impor t 'ddb.docker.libjso nnet '; ddb.Compose( { services : { db : ddb.Image( \"postgres\" ) + ddb.Bi nar y( \"psql\" , \"/project\" , \"psql --dbname=postgresql://postgres:ddb@db/postgres\" ) + ddb.Bi nar y( \"pg_dump\" , \"/project\" , \"pg_dump --dbname=postgresql://postgres:ddb@db/postgres\" ) + { e n viro n me nt + : { POSTGRES_PASSWORD : \"ddb\" }, volumes+ : [ 'db - da ta : /var/lib/pos t gresql/da ta ' , ddb.pa t h.projec t + ' : /projec t ' ] } } } ) Then, run ddb configure , which will generate executable shim file in the .bin folder. Finaly, run $(ddb activate) to update your PATH and bring those commands to your local environment. Now, psql and pg_dump are available as if they were native commands.","title":"Docker binary shims"},{"location":"features/shell/#aliases-management","text":"On your environment, aliases can be manually created in order to save time on repetitive commands execution or on long instructions. Some of those aliases are really useful only in a specific project context. The shell feature allows you to create your own aliases which will be generated the same way as docker binaries are. In order to declare them, you need to update the ddb configuration with the list of aliases : shell : aliases : myAlias : theLongCommandToExecute Make a composer dependency available globally When you are working on PHP/Composer project, some commands are available inside vendor path. For drupal developers, Drush commands is available through vendor/drush/drush/drush binary. Instead of writing the full path of this binary each time, you can declare an alias in ddb configuration. shell : aliases : drush : vendor/drush/drush/drush A binary shim will be created and added to the PATH thanks to ddb so you will able to use it from you project root folder: drush cr","title":"Aliases Management"},{"location":"features/smartcd/","text":"SmartCD \u00b6 The SmartCD feature provides automation to activate/deactive a ddb project environment. Feature configuration (prefixed with smartcd. ) Property Type Description disabled boolean false Should this feature be disabled ? Defaults smartcd : disabled : false SmartCD automation on Linux/Unix \u00b6 Instead of manually launching the command $(ddb run activate) when you are entering a project folder and $(ddb run deactivate) when leaving it, you can install cxreg/smartcd or gfi-centre-ouest/smartcd to automate this process. As developers, we are lazy. So, we have automated generation of .bash_enter and .bash_leave files. With this feature, you can cd from one project to another without thinking about updating environment variable as it does it for you. This feature is enabled only if SmartCD is installed. SmartCD automation on Windows \u00b6 Sadly, we have not found any way to automate updates on environment when entering or leaving a folder on windows environments. As Windows is a bit onerous when it comes to generation of commands to execute, we have automated the generation of two files : ddb_activate.bat and ddb_deactivate.bat in the root folder of your project. Those two files will execute the environment updated commands needed to work on you project and switch to another one. The downside is that you must run them manually.","title":"smartcd"},{"location":"features/smartcd/#smartcd","text":"The SmartCD feature provides automation to activate/deactive a ddb project environment. Feature configuration (prefixed with smartcd. ) Property Type Description disabled boolean false Should this feature be disabled ? Defaults smartcd : disabled : false","title":"SmartCD"},{"location":"features/smartcd/#smartcd-automation-on-linuxunix","text":"Instead of manually launching the command $(ddb run activate) when you are entering a project folder and $(ddb run deactivate) when leaving it, you can install cxreg/smartcd or gfi-centre-ouest/smartcd to automate this process. As developers, we are lazy. So, we have automated generation of .bash_enter and .bash_leave files. With this feature, you can cd from one project to another without thinking about updating environment variable as it does it for you. This feature is enabled only if SmartCD is installed.","title":"SmartCD automation on Linux/Unix"},{"location":"features/smartcd/#smartcd-automation-on-windows","text":"Sadly, we have not found any way to automate updates on environment when entering or leaving a folder on windows environments. As Windows is a bit onerous when it comes to generation of commands to execute, we have automated the generation of two files : ddb_activate.bat and ddb_deactivate.bat in the root folder of your project. Those two files will execute the environment updated commands needed to work on you project and switch to another one. The downside is that you must run them manually.","title":"SmartCD automation on Windows"},{"location":"features/symlinks/","text":"Symlinks \u00b6 This feature is a way to automatically create symlinks in your project. One of the common use we have is project with configurations for each environment (dev, stage, prod). With symlinks feature, depending on environment, the final file is link to the one from environment. Feature configuration (prefixed with symlinks. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? suffixes string[] ['.${core.env.current}'] A list of filename suffix to include. excludes string[] [] A list of glob of filepath to exclude. Advanced Property Type Description includes string[] ['*.${core.env.current}{.*,}'] A list of glob of filepath to include. It is automatically generated from suffixes . Defaults symlinks : disabled : false includes : - '*.dev{.*,}' suffixes : - .dev Symlink creation \u00b6 Bound to events file:found , file:deleted and file:generated , each file retrieved will be compared to lists of configurations includes and excludes to check it is handled or not. If it is a match, symlink will be generated, or deleted if it is a events.file.deleted event. Create .env symlink from .env.prod In many frameworks, we want to have different .env files depending on core.env.current value. Let's say we have created a .env.prod which contains production environment configuration. Instead of manually copying the file or creating a symlink, this will create .env symlink pointing to .env.prod file if core.env.current is set to prod . Tip It can also be chained with any other ddb template generation, such as jinja and ytt","title":"symlinks"},{"location":"features/symlinks/#symlinks","text":"This feature is a way to automatically create symlinks in your project. One of the common use we have is project with configurations for each environment (dev, stage, prod). With symlinks feature, depending on environment, the final file is link to the one from environment. Feature configuration (prefixed with symlinks. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? suffixes string[] ['.${core.env.current}'] A list of filename suffix to include. excludes string[] [] A list of glob of filepath to exclude. Advanced Property Type Description includes string[] ['*.${core.env.current}{.*,}'] A list of glob of filepath to include. It is automatically generated from suffixes . Defaults symlinks : disabled : false includes : - '*.dev{.*,}' suffixes : - .dev","title":"Symlinks"},{"location":"features/symlinks/#symlink-creation","text":"Bound to events file:found , file:deleted and file:generated , each file retrieved will be compared to lists of configurations includes and excludes to check it is handled or not. If it is a match, symlink will be generated, or deleted if it is a events.file.deleted event. Create .env symlink from .env.prod In many frameworks, we want to have different .env files depending on core.env.current value. Let's say we have created a .env.prod which contains production environment configuration. Instead of manually copying the file or creating a symlink, this will create .env symlink pointing to .env.prod file if core.env.current is set to prod . Tip It can also be chained with any other ddb template generation, such as jinja and ytt","title":"Symlink creation"},{"location":"features/traefik/","text":"Traefik \u00b6 One component which we often use on our dev environment is Traefik as reverse proxy. It allows us to run dockerized projects and access them in our browser using project.test as DNS for example. The feature does not install traefik or handle the configuration of your host to map the DNS entry to the IP, but it handles the generation of traefik configuration file for your project if there is certificates for HTTPS access. Feature configuration (prefixed with traefik. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? extra_services dict[string, ExtraService ] [] A dict of ExtraService configuration. Advanced Property Type Description certs_directory string ${core.path.home}/certs Custom certificates location. mapped_certs_directory string ${core.path.home}/certs Traefik container custom certificates location. config_directory string ${core.path.home}/traefik/config Traefik configuration directory. ssl_config_template string <Jinja template> The Jinja template for the traefik configuration file registering CFSSL SSL certificates. This template can be a template string, or a link to a file containing the template, prefixed with http(s):// for web files, or file:// for local ones. extra_services_config_template string <Jinja template> The Jinja template for extra-services configuration file. This template can be a template string, or a link to a file containing the template, prefixed with http(s):// for web files, or file:// for local ones. ExtraService configuration (used in traefik.extra_services ) Simple Property Type Description domain string * Domain to use for SSL certificate generation. url string * URL to access the service to proxy from traefik container. https boolean Use http and/or https to expose the service. If None , it is exposed with both http and https. redirect_to_https boolean ${docker.reverse_proxy.redirect_to_https} If https is None and redirect_to_https is True , requesting the http url of the service will reply with a temporary redirect to https. Advanced Property Type Description path_prefix string The traefik prefix path. You should customize it only if you have to support sub folder on the domain. redirect_to_path_prefix boolean The traefik prefix path. You should customize it only if you have to support sub folder on the domain. rule string Host(`{{_local.domain}}`) Custom certificates location. Defaults traefik : certs_directory : /home/toilal/.docker-devbox/certs mapped_certs_directory : /certs config_directory : /home/toilal/.docker-devbox/traefik/config disabled : false extra_services : {} extra_services_config_template : \"# This configuration file has been automatically\\ \\ generated by ddb\\n[http.routers]\\n{%- if _local.https is none or _local.https\\ \\ is sameas false %}\\n [http.routers.extra-service-{{_local.id}}]\\n rule =\\ \\ \\\"{{_local.rule}}\\\"{% if not _local.redirect_to_https and _local.path_prefix\\ \\ %} && \\\"PathPrefix(`{{_local.path_prefix}}{regex:$$|/.*}`)\\\"{% endif %}\\n \\ \\ entrypoints = [\\\"http\\\"]\\n service = \\\"extra-service-{{_local.id}}\\\"\\n{%-\\ \\ if _local.redirect_to_https %}\\n middlewares = [\\\"extra-service-{{_local.id}}-redirect-to-https\\\"\\ ]\\n{%- elif _local.path_prefix %}\\n middlewares = [\\\"extra-service-{{_local.id}}-stripprefix\\\"\\ ]\\n{%- endif %}\\n{%- endif %}\\n{%- if _local.https is none or _local.https is\\ \\ sameas true %}\\n [http.routers.extra-service-{{_local.id}}-tls]\\n rule =\\ \\ \\\"{{_local.rule}}{% if _local.path_prefix %} && PathPrefix(`{{_local.path_prefix}}{regex:$$|/.*}`){%\\ \\ endif %}\\\"\\n entrypoints = [\\\"https\\\"]\\n tls = true\\n service = \\\"\\ extra-service-{{_local.id}}\\\"\\n{%- if _local.path_prefix %}\\n middlewares =\\ \\ [\\\"extra-service-{{_local.id}}-stripprefix\\\"]\\n{%- endif %}\\n{%- if _local.certresolver\\ \\ is defined %}\\n [http.routers.extra-service-{{_local.service}}-tls.tls]\\n\\ \\ certResolver = \\\"{{_local.certresolver}}\\\"\\n{%- endif %}\\n{%- endif %}\\n\\ {%- if _local.redirect_to_path_prefix %}\\n{%- if _local.https is none and not\\ \\ _local.redirect_to_https or _local.https is sameas false %}\\n [http.routers.extra-service-{{_local.id}}-redirect-to-path-prefix]\\n\\ \\ rule = \\\"{{_local.rule}}\\\"\\n entrypoints = [\\\"http\\\"]\\n service = \\\"\\ extra-service-{{_local.id}}\\\"\\n middlewares = [\\\"extra-service-{{_local.id}}-redirect-to-path-prefix\\\"\\ ]\\n{%- endif %}\\n{%- if _local.https is none or _local.https is sameas true %}\\n\\ \\ [http.routers.extra-service-{{_local.id}}-redirect-to-path-prefix-tls]\\n \\ \\ rule = \\\"{{_local.rule}}\\\"\\n entrypoints = [\\\"https\\\"]\\n tls = true\\n\\ \\ service = \\\"extra-service-{{_local.id}}\\\"\\n middlewares = [\\\"extra-service-{{_local.id}}-redirect-to-path-prefix\\\"\\ ]\\n{%- if _local.certresolver is defined %}\\n [http.routers.extra-service-{{_local.service}}-tls.tls]\\n\\ \\ certResolver = \\\"{{_local.certresolver}}\\\"\\n{%- endif %}\\n{%- endif %}\\n\\ {%- endif %}\\n\\n{%- if _local.redirect_to_https or _local.path_prefix %}\\n\\n\\ [http.middlewares]\\n{%- if _local.redirect_to_https %}\\n [http.middlewares.extra-service-{{_local.id}}-redirect-to-https.redirectScheme]\\n\\ \\ scheme = \\\"https\\\"\\n{%- endif %}\\n{%- if _local.path_prefix %}\\n [http.middlewares.extra-service-{{_local.id}}-stripprefix.stripPrefix]\\n\\ \\ prefixes = \\\"{{_local.path_prefix}}\\\"\\n{%- endif %}\\n{%- if _local.redirect_to_path_prefix\\ \\ %}\\n [http.middlewares.extra-service-{{_local.id}}-redirect-to-path-prefix.redirectregex]\\n\\ \\ regex = \\\"^.*$\\\"\\n replacement = \\\"{{_local.path_prefix}}\\\"\\n{%- endif\\ \\ %}\\n{%- endif %}\\n\\n[http.services]\\n [http.services.extra-service-{{_local.id}}]\\n\\ \\ [http.services.extra-service-{{_local.id}}.loadBalancer]\\n [[http.services.extra-service-{{_local.id}}.loadBalancer.servers]]\\n\\ \\ url = \\\"{{_local.url}}\\\"\\n\\n\" ssl_config_template : \"# This configuration file has been automatically generated\\ \\ by ddb\\n[[tls.certificates]]\\n certFile = \\\"{{_local.certFile}}\\\"\\n keyFile\\ \\ = \\\"{{_local.keyFile}}\\\"\\n\\n\" Include a service running outside of docker compose \u00b6 If you need to register an external service into your docker network, you should define an external_service entry. It can be used when a service is running outside the docker network, like on another Machine or on the developer host. Declared extra services make them join the docker network so it behaves like a docker compose service and brings all traefik reverse-proxy features (SSL support, domain name, ...). Jinja templating is available for url , domain and rule fields, with the usual configuration as data context, and additional _local dict containing the extra_service entry configuration. Bring back a dockerized service inside your IDE Running the server component inside the developer editor may more convenient in some cases. If the application is running on port 8080 right on the host, you can write this kind of configuration: traefik : extra_services : api : domain : api.{{core.domain.sub}}.{{core.domain.ext}} url : http://{{docker.debug.host}}:8080 It will expose the server component throw the traefik docker network, with the domain name and HTTPS support. Custom certificates feature \u00b6 If your project have certificates for SSL access, Traefik needs a bit a configuration in order to use them. This is done on ddb configure command. For instance, if your have set docker.reverse-proxy.certresolver with null value in your docker-compose.yml.jsonnet (check feature jsonnet for more details), it will create a label ddb.emit.certs:generate: <domain> . This label emit a certs:generate for given domain , and it is processed by certs feature to generate a custom SSL certificate. Then, certs:available event is triggered and handled by traefik feature to install this certificate in the traefik configuration for given domain . For some reason, you might want to remove HTTPS on your project and move back to HTTP. This is done on ddb configure command. If you define docker.reverse-proxy.certresolver value to 'letsencrypt', or set traefik.https to False , it is detected that you removed it. The certs:remove event is then triggered and handled by certs feature to remove it. Then, certs:removed event is triggered and handler by traefik feature to uninstall this certificate from traefik configuration, so there's no more certificate defined for the given domain.","title":"traefik"},{"location":"features/traefik/#traefik","text":"One component which we often use on our dev environment is Traefik as reverse proxy. It allows us to run dockerized projects and access them in our browser using project.test as DNS for example. The feature does not install traefik or handle the configuration of your host to map the DNS entry to the IP, but it handles the generation of traefik configuration file for your project if there is certificates for HTTPS access. Feature configuration (prefixed with traefik. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? extra_services dict[string, ExtraService ] [] A dict of ExtraService configuration. Advanced Property Type Description certs_directory string ${core.path.home}/certs Custom certificates location. mapped_certs_directory string ${core.path.home}/certs Traefik container custom certificates location. config_directory string ${core.path.home}/traefik/config Traefik configuration directory. ssl_config_template string <Jinja template> The Jinja template for the traefik configuration file registering CFSSL SSL certificates. This template can be a template string, or a link to a file containing the template, prefixed with http(s):// for web files, or file:// for local ones. extra_services_config_template string <Jinja template> The Jinja template for extra-services configuration file. This template can be a template string, or a link to a file containing the template, prefixed with http(s):// for web files, or file:// for local ones. ExtraService configuration (used in traefik.extra_services ) Simple Property Type Description domain string * Domain to use for SSL certificate generation. url string * URL to access the service to proxy from traefik container. https boolean Use http and/or https to expose the service. If None , it is exposed with both http and https. redirect_to_https boolean ${docker.reverse_proxy.redirect_to_https} If https is None and redirect_to_https is True , requesting the http url of the service will reply with a temporary redirect to https. Advanced Property Type Description path_prefix string The traefik prefix path. You should customize it only if you have to support sub folder on the domain. redirect_to_path_prefix boolean The traefik prefix path. You should customize it only if you have to support sub folder on the domain. rule string Host(`{{_local.domain}}`) Custom certificates location. Defaults traefik : certs_directory : /home/toilal/.docker-devbox/certs mapped_certs_directory : /certs config_directory : /home/toilal/.docker-devbox/traefik/config disabled : false extra_services : {} extra_services_config_template : \"# This configuration file has been automatically\\ \\ generated by ddb\\n[http.routers]\\n{%- if _local.https is none or _local.https\\ \\ is sameas false %}\\n [http.routers.extra-service-{{_local.id}}]\\n rule =\\ \\ \\\"{{_local.rule}}\\\"{% if not _local.redirect_to_https and _local.path_prefix\\ \\ %} && \\\"PathPrefix(`{{_local.path_prefix}}{regex:$$|/.*}`)\\\"{% endif %}\\n \\ \\ entrypoints = [\\\"http\\\"]\\n service = \\\"extra-service-{{_local.id}}\\\"\\n{%-\\ \\ if _local.redirect_to_https %}\\n middlewares = [\\\"extra-service-{{_local.id}}-redirect-to-https\\\"\\ ]\\n{%- elif _local.path_prefix %}\\n middlewares = [\\\"extra-service-{{_local.id}}-stripprefix\\\"\\ ]\\n{%- endif %}\\n{%- endif %}\\n{%- if _local.https is none or _local.https is\\ \\ sameas true %}\\n [http.routers.extra-service-{{_local.id}}-tls]\\n rule =\\ \\ \\\"{{_local.rule}}{% if _local.path_prefix %} && PathPrefix(`{{_local.path_prefix}}{regex:$$|/.*}`){%\\ \\ endif %}\\\"\\n entrypoints = [\\\"https\\\"]\\n tls = true\\n service = \\\"\\ extra-service-{{_local.id}}\\\"\\n{%- if _local.path_prefix %}\\n middlewares =\\ \\ [\\\"extra-service-{{_local.id}}-stripprefix\\\"]\\n{%- endif %}\\n{%- if _local.certresolver\\ \\ is defined %}\\n [http.routers.extra-service-{{_local.service}}-tls.tls]\\n\\ \\ certResolver = \\\"{{_local.certresolver}}\\\"\\n{%- endif %}\\n{%- endif %}\\n\\ {%- if _local.redirect_to_path_prefix %}\\n{%- if _local.https is none and not\\ \\ _local.redirect_to_https or _local.https is sameas false %}\\n [http.routers.extra-service-{{_local.id}}-redirect-to-path-prefix]\\n\\ \\ rule = \\\"{{_local.rule}}\\\"\\n entrypoints = [\\\"http\\\"]\\n service = \\\"\\ extra-service-{{_local.id}}\\\"\\n middlewares = [\\\"extra-service-{{_local.id}}-redirect-to-path-prefix\\\"\\ ]\\n{%- endif %}\\n{%- if _local.https is none or _local.https is sameas true %}\\n\\ \\ [http.routers.extra-service-{{_local.id}}-redirect-to-path-prefix-tls]\\n \\ \\ rule = \\\"{{_local.rule}}\\\"\\n entrypoints = [\\\"https\\\"]\\n tls = true\\n\\ \\ service = \\\"extra-service-{{_local.id}}\\\"\\n middlewares = [\\\"extra-service-{{_local.id}}-redirect-to-path-prefix\\\"\\ ]\\n{%- if _local.certresolver is defined %}\\n [http.routers.extra-service-{{_local.service}}-tls.tls]\\n\\ \\ certResolver = \\\"{{_local.certresolver}}\\\"\\n{%- endif %}\\n{%- endif %}\\n\\ {%- endif %}\\n\\n{%- if _local.redirect_to_https or _local.path_prefix %}\\n\\n\\ [http.middlewares]\\n{%- if _local.redirect_to_https %}\\n [http.middlewares.extra-service-{{_local.id}}-redirect-to-https.redirectScheme]\\n\\ \\ scheme = \\\"https\\\"\\n{%- endif %}\\n{%- if _local.path_prefix %}\\n [http.middlewares.extra-service-{{_local.id}}-stripprefix.stripPrefix]\\n\\ \\ prefixes = \\\"{{_local.path_prefix}}\\\"\\n{%- endif %}\\n{%- if _local.redirect_to_path_prefix\\ \\ %}\\n [http.middlewares.extra-service-{{_local.id}}-redirect-to-path-prefix.redirectregex]\\n\\ \\ regex = \\\"^.*$\\\"\\n replacement = \\\"{{_local.path_prefix}}\\\"\\n{%- endif\\ \\ %}\\n{%- endif %}\\n\\n[http.services]\\n [http.services.extra-service-{{_local.id}}]\\n\\ \\ [http.services.extra-service-{{_local.id}}.loadBalancer]\\n [[http.services.extra-service-{{_local.id}}.loadBalancer.servers]]\\n\\ \\ url = \\\"{{_local.url}}\\\"\\n\\n\" ssl_config_template : \"# This configuration file has been automatically generated\\ \\ by ddb\\n[[tls.certificates]]\\n certFile = \\\"{{_local.certFile}}\\\"\\n keyFile\\ \\ = \\\"{{_local.keyFile}}\\\"\\n\\n\"","title":"Traefik"},{"location":"features/traefik/#include-a-service-running-outside-of-docker-compose","text":"If you need to register an external service into your docker network, you should define an external_service entry. It can be used when a service is running outside the docker network, like on another Machine or on the developer host. Declared extra services make them join the docker network so it behaves like a docker compose service and brings all traefik reverse-proxy features (SSL support, domain name, ...). Jinja templating is available for url , domain and rule fields, with the usual configuration as data context, and additional _local dict containing the extra_service entry configuration. Bring back a dockerized service inside your IDE Running the server component inside the developer editor may more convenient in some cases. If the application is running on port 8080 right on the host, you can write this kind of configuration: traefik : extra_services : api : domain : api.{{core.domain.sub}}.{{core.domain.ext}} url : http://{{docker.debug.host}}:8080 It will expose the server component throw the traefik docker network, with the domain name and HTTPS support.","title":"Include a service running outside of docker compose"},{"location":"features/traefik/#custom-certificates-feature","text":"If your project have certificates for SSL access, Traefik needs a bit a configuration in order to use them. This is done on ddb configure command. For instance, if your have set docker.reverse-proxy.certresolver with null value in your docker-compose.yml.jsonnet (check feature jsonnet for more details), it will create a label ddb.emit.certs:generate: <domain> . This label emit a certs:generate for given domain , and it is processed by certs feature to generate a custom SSL certificate. Then, certs:available event is triggered and handled by traefik feature to install this certificate in the traefik configuration for given domain . For some reason, you might want to remove HTTPS on your project and move back to HTTP. This is done on ddb configure command. If you define docker.reverse-proxy.certresolver value to 'letsencrypt', or set traefik.https to False , it is detected that you removed it. The certs:remove event is then triggered and handled by certs feature to remove it. Then, certs:removed event is triggered and handler by traefik feature to uninstall this certificate from traefik configuration, so there's no more certificate defined for the given domain.","title":"Custom certificates feature"},{"location":"features/version/","text":"Version \u00b6 Version feature extracts information about the version of your project from local git repository. Feature configuration (prefixed with version. ) Property Type Description disabled boolean false Should this feature be disabled ? branch string <current git branch> The current branch of the project. hash string <current git hash> The hash of the current commit. short_hash string <current git short hash> The short version of the hash of the current commit. tag string <current git tag> The current git tag. version string <current project version> The current project version. Defaults version : branch : master disabled : false hash : c8a21135dab1fbb3b994bc2d5a374e2f5477ddfa short_hash : c8a2113 tag : null version : null","title":"version"},{"location":"features/version/#version","text":"Version feature extracts information about the version of your project from local git repository. Feature configuration (prefixed with version. ) Property Type Description disabled boolean false Should this feature be disabled ? branch string <current git branch> The current branch of the project. hash string <current git hash> The hash of the current commit. short_hash string <current git short hash> The short version of the hash of the current commit. tag string <current git tag> The current git tag. version string <current project version> The current project version. Defaults version : branch : master disabled : false hash : c8a21135dab1fbb3b994bc2d5a374e2f5477ddfa short_hash : c8a2113 tag : null version : null","title":"Version"},{"location":"features/ytt/","text":"ytt \u00b6 YTT is a template engine dedicated to yaml. Feature configuration (prefixed with ytt. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? suffixes string[] ['.ytt'] A list of filename suffix to include. extensions string[] ['yaml', 'yml', ''] A list of glob of supported extension. excludes string[] [] A list of glob of filepath to exclude. args string[] [] A list of arguments to pass to ytt. depends_suffixes string[] ['.data', '.overlay'] File suffix to use for ytt dependency files. Advanced Property Type Description includes string[] ['*.ytt{.yaml,.yml,}'] A list of glob of filepath to include. It is automatically generated from suffixes and extensions . keywords string[] keywords_escape_format string[] %s_ Defaults ytt : disabled : false args : - --ignore-unknown-comments depends_suffixes : - .data - .overlay extensions : - .yaml - .yml - '' includes : - '*.ytt{.yaml,.yml,}' keywords : - and - elif - in - or - break - else - lambda - pass - continue - for - load - return - def - if - not - while - as - finally - nonlocal - assert - from - raise - class - global - try - del - import - with - except - is - yield keywords_escape_format : '%s_' suffixes : - .ytt","title":"ytt"},{"location":"features/ytt/#ytt","text":"YTT is a template engine dedicated to yaml. Feature configuration (prefixed with ytt. ) Simple Property Type Description disabled boolean false Should this feature be disabled ? suffixes string[] ['.ytt'] A list of filename suffix to include. extensions string[] ['yaml', 'yml', ''] A list of glob of supported extension. excludes string[] [] A list of glob of filepath to exclude. args string[] [] A list of arguments to pass to ytt. depends_suffixes string[] ['.data', '.overlay'] File suffix to use for ytt dependency files. Advanced Property Type Description includes string[] ['*.ytt{.yaml,.yml,}'] A list of glob of filepath to include. It is automatically generated from suffixes and extensions . keywords string[] keywords_escape_format string[] %s_ Defaults ytt : disabled : false args : - --ignore-unknown-comments depends_suffixes : - .data - .overlay extensions : - .yaml - .yml - '' includes : - '*.ytt{.yaml,.yml,}' keywords : - and - elif - in - or - break - else - lambda - pass - continue - for - load - return - def - if - not - while - as - finally - nonlocal - assert - from - raise - class - global - try - del - import - with - except - is - yield keywords_escape_format : '%s_' suffixes : - .ytt","title":"ytt"},{"location":"guides/psql-symfony-vue/","text":"Guide \u00b6 PostgreSQL, Symfony, VueJS This guide sources are available on github . Create empty project directory \u00b6 First of all, you need an empty directory for your project. mkdir ddb-guide cd ddb-guide Setup database \u00b6 You should now setup the database container. Create docker-compose.yml.jsonnet file, and add the following content: local ddb = impor t 'ddb.docker.libjso nnet '; ddb.Compose( { services : { db : ddb.Image( \"postgres\" ) } } ) Jsonet, a data templating language Instead of defining containers right inside docker-compose.yml with yaml, ddb is using Jsonnet , a data templating language. Inside the jsonnet file, a library is imported to bring handy features and consistent behavior for all containers while reducing verbosity. Jsonet is embedded into ddb Jsonnet is embedded into ddb. You only have to use the right file extension for ddb to process it through the appropriate template engine. Other template languages are supported ddb embeds other templating languages, like Jinja and ytt . But for building the docker-compose.yml file, Jsonnet is the best choice and brings access to all features from ddb. Run the ddb configure command ddb configure Commands ddb is a command line tool, and implements many commands. configure is the main one. It configures the whole project based on available files in the project directory. docker-compose.yml file has been generated. networks : {} services : db : image : postgres init : true restart : 'no' version : '3.7' volumes : {} .gitignore automation for generated files You may have noticed that a .gitignore has also been generated, to exclude docker-compose.yml . ddb may generates many files from templates. When ddb generates a file, it will always be added to the .gitignore file. Launch the stack with docker-compose, and check database logs. docker-compose up -d docker-compose logs db Sadly, there's an error in logs and container has stopped. You only have to define a database password with environment variable POSTGRES_PASSWORD . Add this environment variable to docker-compose.yml.jsonnet template. local ddb = impor t 'ddb.docker.libjso nnet '; ddb.Compose( { services : { db : ddb.Image( \"postgres\" ) + { e n viro n me nt + : { POSTGRES_PASSWORD : \"ddb\" } } } } ) Jsonnet You may feel uncomfortable at first with Jsonnet , but this is a great tool and it brings a huge value to ddb. Here, we are merging the json object returned by ddb.Image(\"postgres\") with another object containing an environment key with environment variable values. + behind environment key name means that values from the new object are appended to values from the source one, instead of beeing replaced. To fully understand syntax and capabilities of jsonnet, you should take time to learn it . Run configure command again. ddb configure The generated docker-compose.yml file should now look like this: networks : {} services : db : environment : POSTGRES_PASSWORD : ddb image : postgres init : true restart : 'no' version : '3.7' volumes : {} docker-compose up -d docker-compose logs db The database should be up now ! Great :) Start watch mode \u00b6 As you now understand how ddb basics works, you may feel that running the ddb configure after each change is annoying. I'm pretty sure you want to stay a happy developer, so open a new interpreter, cd inside project directory, and run configure command with --watch flag. ddb --watch configure ddb is now running and listening for file events inside the project. Try to change database password inside the docker-compose.yml.jsonnet template file, it will immediately refresh docker-compose.yml . That's what we call Watch mode . Add a named volume \u00b6 As you may already know, you need to setup a volume for data to be persisted if the container is destroyed. Let's stop and destroy all containers for now. docker-compose down Map a named volume to the db service inside docker-compose.yml.jsonnet . local ddb = impor t 'ddb.docker.libjso nnet '; ddb.Compose( { services : { db : ddb.Image( \"postgres\" ) { e n viro n me nt + : { POSTGRES_PASSWORD : \"ddb\" }, volumes+ : [ 'db - da ta : /var/lib/pos t gresql/da ta ' ] } } } ) Thanks to watch mode, changes are immediately generated in docker-compose.yml file networks : {} services : db : environment : POSTGRES_PASSWORD : ddb image : postgres init : true restart : 'no' volumes : - db-data:/var/lib/postgresql/data version : '3.7' volumes : db-data : {} db-data volume is now mapped on /var/lib/postgresql/data inside db service. And db-data volume has also been declared in the main volumes section ! Magic :) In fact, it's not so magic Those automated behavior provided by docker-compose.yml.jsonnet , like init and restart on each service, and global volumes declaration, are handled by ddb jsonnet library through ddb.Compose() function. For more information, check Jsonnet Feature section. Register binary from image \u00b6 Database docker image contains binaries that you may need to run, like ... psql - PostgreSQL client binary. pg_dump - PostgreSQL command line tool to export data to a file. With docker-compose or raw docker, it may be really painful to find out the right docker-compose command to run those binary from your local environment. You may also have issues to read input data file, or to write output data file. With ddb, you can register those binaries right into docker-compose.yml.jsonnet to make them accessible from your local environment. local ddb = impor t 'ddb.docker.libjso nnet '; ddb.Compose( { services : { db : ddb.Image( \"postgres\" ) + ddb.Bi nar y( \"psql\" , \"/project\" , \"psql --dbname=postgresql://postgres:ddb@db/postgres\" ) + ddb.Bi nar y( \"pg_dump\" , \"/project\" , \"pg_dump --dbname=postgresql://postgres:ddb@db/postgres\" ) + { e n viro n me nt + : { POSTGRES_PASSWORD : \"ddb\" }, volumes+ : [ 'db - da ta : /var/lib/pos t gresql/da ta ' , ddb.pa t h.projec t + ' : /projec t ' ] } } } ) You should notice that some binary files have been generated in .bin directory : psql and pg_dump . Those binaries are available right now on your local environment. You can check their version. .bin/psql --version .bin/pg_dump --version But ... What the ... Where is all the docker hard stuff ? Of course, the docker hard stuff is still there. But it's hidden. ddb generates some shims for those binaries available in docker image, so you fell like those binaries are now installed on the project. But when invoking those binary shims, it creates a temporary container for the command's lifetime. Current working directory is mapped When registering binary from jsonnet this way, the project directory on your local environment should be mounted to /project inside the container. The working directory of the container is mapped to the working directory of your local environment. This allow ddb to match working directory from local environment and container, so you are able to access any files through a natural process. Default arguments --dbname=postgresql://postgres:ddb@db/postgres is added as a default argument to both command, so the commands won't require any connection settings. To bring psql and pg_dump shims into the path, you have to activate the project environment into your interpreter. $( ddb activate ) .bin directory is now in the interpreter's PATH , so psql and pg_dump are available anywhere. psql --version pg_dump --version Let's try to perform a dump with pg_dump . pg_dump -f dump.sql Great, the dump.sql file appears in your working directory ! Perfect. But if you check carefully your project directory, there's a problem here ! The dump file has been generated, but it is owned by root . You are not allowed to write or delete this file now. Thanks to sudo , you can do still delete it. sudo rm dump.sql But this suck ... As a developer, you are really disappointed ... And you are right. Nobody wants a file to be owned by root inside the project directory. Workaround permission issues \u00b6 To workaround those permission issues, ddb has automated the installation of fixuid inside a Dockerfile. Docker and permission issues Permission issues are a common pitfall while using docker on development environments. They are related to the way docker works and cannot really be fixed once for all. As you know, ddb like templates, so you are going to use Jinja for all Dockerfile files. By convention, custom Dockerfile.jinja lies in .docker/<image> directory, where <image> is to be replaced with effective image name. First step, create .docker/postgres/Dockerfile.jinja from postgres base image. FROM postgres USER postgres Second step, create .docker/postgres/fixuid.yml file. user : postgres group : postgres paths : - / - /var/lib/postgresql/data Fixuid Fixuid change uid and gid of the container user to match the host user, and it changes files ownerships as declared in fixuid.yml configuration file. Most of the time, user and group defined in the configuration file should match the user defined in Dockerfile, and paths should match the root directory and volume directories. When a fixuid.yml file is available next to a Dockerfile, ddb generates fixuid installation instructions into the Dockerfile , and entrypoint is changed to run fixuid before the default entrypoint. Last step, change in docker-compose.yml.jsonnet the service definition to use the newly created Dockerfile ( ddb.Image(\"postgres\") replaced to ddb.Build(\"postgres\") ), and set user to the host user uid/gid ( ddb.User() ). local ddb = impor t 'ddb.docker.libjso nnet '; ddb.Compose( { services : { db : ddb.Build( \"postgres\" ) + ddb.User() + ddb.Bi nar y( \"psql\" , \"/project\" , \"psql --dbname=postgresql://postgres:ddb@db/postgres\" ) + ddb.Bi nar y( \"pg_dump\" , \"/project\" , \"pg_dump --dbname=postgresql://postgres:ddb@db/postgres\" ) + { e n viro n me nt + : { POSTGRES_PASSWORD : \"ddb\" }, volumes+ : [ 'db - da ta : /var/lib/pos t gresql/da ta ' , ddb.pa t h.projec t + ' : /projec t ' ] } } } ) Stop containers, destroy data from existing database, and start again. docker-compose down -v docker-compose up -d Perform the dump. pg_dump -f dump.sql dump.sql is now owned by your own user, and as a developer, you are happy again :) Setup PHP, Apache and Symfony Skeleton \u00b6 Then, we need to setup PHP FPM with it's related web server Apache. So, we are creating a new php service inside docker-compose.yml.jsonnet , based on a Dockerfile build. local ddb = impor t 'ddb.docker.libjso nnet '; ddb.Compose( { services : { ... php : ddb.Build( \"php\" ) + ddb.User() + { volumes+ : [ ddb.pa t h.projec t + \":/var/www/html\" , \"php-composer-cache:/composer/cache\" , \"php-composer-vendor:/composer/vendor\" ] } } ) And the related Dockerfile.jinja inside .docker/php directory. FROM php:7.3-fpm RUN docker-php-ext-install opcache RUN yes | pecl install xdebug && docker-php-ext-enable xdebug RUN apt-get update -y && \\ apt-get install -y libpq-dev && \\ rm -rf /var/lib/apt/lists/* && \\ docker-php-ext-install pdo pdo_pgsql ENV COMPOSER_HOME /composer ENV PATH /composer/vendor/bin: $PATH ENV COMPOSER_ALLOW_SUPERUSER 1 COPY --from = composer:2 /usr/bin/composer /usr/bin/composer RUN apt-get update -y && \\ apt-get install -y git zip unzip && \\ rm -rf /var/lib/apt/lists/* RUN mkdir -p \" $COMPOSER_HOME /cache\" \\ && mkdir -p \" $COMPOSER_HOME /vendor\" \\ && chown -R www-data:www-data $COMPOSER_HOME \\ && chown -R www-data:www-data /var/www VOLUME /composer/cache And fixuid.yml to fix file permission issues. user : www-data group : www-data paths : - / - /composer/cache Then build the docker image with docker-compose build . Composer has been installed in the image, so let's make it available by registering a binary into docker-compose.yml.jsonnet . We can also register the php binary for it to be available locally too. local ddb = import 'ddb.docker.libjsonnet'; ddb.Compose({ services: { ... php: ddb.Build(\"php\") + ddb.User() + ddb.Binary(\"composer\", \"/var/www/html\", \"composer\") + ddb.Binary(\"php\", \"/var/www/html\", \"php\") + { volumes+: [ ddb.path.project + \":/var/www/html\", \"php-composer-cache:/composer/cache\", \"php-composer-vendor:/composer/vendor\" ] } }, }) And activate the project, with $(ddb activate) . The composer command in now available right in your PATH. $ composer --version Composer version 1 .10.10 2020 -08-03 11 :35:19 $ php --version PHP 7 .3.10 ( cli ) ( built: Oct 17 2019 15 :09:28 ) ( NTS ) Copyright ( c ) 1997 -2018 The PHP Group Zend Engine v3.3.10, Copyright ( c ) 1998 -2018 Zend Technologies with Xdebug v2.9.6, Copyright ( c ) 2002 -2020, by Derick Rethans Now PHP and composer are available, you can generate the symfony skeleton inside a backend directory. composer create-project symfony/skeleton backend We also need a web service for Apache configured with PHP. Here's the docker-compose.yml.jsonnet part. local ddb = impor t 'ddb.docker.libjso nnet '; local domai n _ex t = s t d.ex t Var( \"core.domain.ext\" ); local domai n _sub = s t d.ex t Var( \"core.domain.sub\" ); local domai n = s t d.joi n ('.' , [ domai n _sub , domai n _ex t ] ); ddb.Compose( { services : { ... web : ddb.Build( \"web\" ) + ddb.Vir tual Hos t ( \"80\" , domai n ) { volumes+ : [ ddb.pa t h.projec t + \":/var/www/html\" , ddb.pa t h.projec t + \"/.docker/web/apache.conf:/usr/local/apache2/conf/custom/apache.conf\" , ] }, } ) Use std.extVar(...) inside jsonnet to read a configuration property As you can see here, we are using jsonnet features to build the domain name and setup the traefik configuration for the virtualhost. Configuration properties are available inside all template engines and can be listed with ddb config --variables As with the php service, a .docker/web/Dockerfile.jinja is created to define the image build. FROM httpd:2.4 RUN mkdir -p /usr/local/apache2/conf/custom \\ && mkdir -p /var/www/html \\ && sed -i '/LoadModule proxy_module/s/^#//g' /usr/local/apache2/conf/httpd.conf \\ && sed -i '/LoadModule proxy_fcgi_module/s/^#//g' /usr/local/apache2/conf/httpd.conf \\ && echo >> /usr/local/apache2/conf/httpd.conf && echo 'Include conf/custom/*.conf' >> /usr/local/apache2/conf/httpd.conf RUN sed -i '/LoadModule headers_module/s/^#//g' /usr/local/apache2/conf/httpd.conf RUN sed -i '/LoadModule rewrite_module/s/^#//g' /usr/local/apache2/conf/httpd.conf apache.conf specified in docker compose volume mount is also generated from a jinja file, apache.conf.jinja . It is used to inject domain name and docker compose network name, for the domain to be centralized into ddb.yml configuration and ease various environment deployements (stage, prod). <VirtualHost *:80> ServerAdmin webmaster@{{core.domain.sub}}.{{core.domain.ext}} ServerName api.{{core.domain.sub}}.{{core.domain.ext}} DocumentRoot /var/www/html/backend/public <Directory \"/var/www/html/backend/public/\"> DirectoryIndex index.php AllowOverride All Order allow,deny Allow from all Require all granted # symfony configuration from https://github.com/symfony/recipes-contrib/blob/master/symfony/apache-pack/1.0/public/.htaccess # By default, Apache does not evaluate symbolic links if you did not enable this # feature in your server configuration. Uncomment the following line if you # install assets as symlinks or if you experience problems related to symlinks # when compiling LESS/Sass/CoffeScript assets. # Options FollowSymlinks # Disabling MultiViews prevents unwanted negotiation, e.g. \"/index\" should not resolve # to the front controller \"/index.php\" but be rewritten to \"/index.php/index\". <IfModule mod_negotiation.c> Options -MultiViews </IfModule> <IfModule mod_rewrite.c> RewriteEngine On # Determine the RewriteBase automatically and set it as environment variable. # If you are using Apache aliases to do mass virtual hosting or installed the # project in a subdirectory, the base path will be prepended to allow proper # resolution of the index.php file and to redirect to the correct URI. It will # work in environments without path prefix as well, providing a safe, one-size # fits all solution. But as you do not need it in this case, you can comment # the following 2 lines to eliminate the overhead. RewriteCond %{REQUEST_URI}::$1 ^(/.+)/(.*)::\\2$ RewriteRule ^(.*) - [E=BASE:%1] # Sets the HTTP_AUTHORIZATION header removed by Apache RewriteCond %{HTTP:Authorization} . RewriteRule ^ - [E=HTTP_AUTHORIZATION:%{HTTP:Authorization}] # Redirect to URI without front controller to prevent duplicate content # (with and without `/index.php`). Only do this redirect on the initial # rewrite by Apache and not on subsequent cycles. Otherwise we would get an # endless redirect loop (request -> rewrite to front controller -> # redirect -> request -> ...). # So in case you get a \"too many redirects\" error or you always get redirected # to the start page because your Apache does not expose the REDIRECT_STATUS # environment variable, you have 2 choices: # - disable this feature by commenting the following 2 lines or # - use Apache >= 2.3.9 and replace all L flags by END flags and remove the # following RewriteCond (best solution) RewriteCond %{ENV:REDIRECT_STATUS} ^$ RewriteRule ^index\\.php(?:/(.*)|$) %{ENV:BASE}/$1 [R=301,L] # If the requested filename exists, simply serve it. # We only want to let Apache serve files and not directories. RewriteCond %{REQUEST_FILENAME} -f RewriteRule ^ - [L] # Rewrite all other queries to the front controller. RewriteRule ^ %{ENV:BASE}/index.php [L] </IfModule> <IfModule !mod_rewrite.c> <IfModule mod_alias.c> # When mod_rewrite is not available, we instruct a temporary redirect of # the start page to the front controller explicitly so that the website # and the generated links can still be used. RedirectMatch 307 ^/$ /index.php/ # RedirectTemp cannot be used instead </IfModule> </IfModule> </Directory> SetEnvIf Authorization \"(.*)\" HTTP_AUTHORIZATION=$1 <FilesMatch \\.php$> SetHandler \"proxy:fcgi://php.{{docker.compose.network_name}}:9000\" </FilesMatch> </VirtualHost> And now, we are ready start all containers : docker-compose up -d . Run ddb info command to check the URL of your virtualhost for the web service. You should be able to view Symfony landing page at http://ddb-quickstart.test and https://ddb-quickstart.test . You may have to restart traefik container If you have some issues with certificate validity on the https:// url, you may need to restart traefik container : docker restart traefik . Setup VueJS and Vue CLI \u00b6 To be continued","title":"PostgreSQL, Symfony, VueJS"},{"location":"guides/psql-symfony-vue/#guide","text":"PostgreSQL, Symfony, VueJS This guide sources are available on github .","title":"Guide"},{"location":"guides/psql-symfony-vue/#create-empty-project-directory","text":"First of all, you need an empty directory for your project. mkdir ddb-guide cd ddb-guide","title":"Create empty project directory"},{"location":"guides/psql-symfony-vue/#setup-database","text":"You should now setup the database container. Create docker-compose.yml.jsonnet file, and add the following content: local ddb = impor t 'ddb.docker.libjso nnet '; ddb.Compose( { services : { db : ddb.Image( \"postgres\" ) } } ) Jsonet, a data templating language Instead of defining containers right inside docker-compose.yml with yaml, ddb is using Jsonnet , a data templating language. Inside the jsonnet file, a library is imported to bring handy features and consistent behavior for all containers while reducing verbosity. Jsonet is embedded into ddb Jsonnet is embedded into ddb. You only have to use the right file extension for ddb to process it through the appropriate template engine. Other template languages are supported ddb embeds other templating languages, like Jinja and ytt . But for building the docker-compose.yml file, Jsonnet is the best choice and brings access to all features from ddb. Run the ddb configure command ddb configure Commands ddb is a command line tool, and implements many commands. configure is the main one. It configures the whole project based on available files in the project directory. docker-compose.yml file has been generated. networks : {} services : db : image : postgres init : true restart : 'no' version : '3.7' volumes : {} .gitignore automation for generated files You may have noticed that a .gitignore has also been generated, to exclude docker-compose.yml . ddb may generates many files from templates. When ddb generates a file, it will always be added to the .gitignore file. Launch the stack with docker-compose, and check database logs. docker-compose up -d docker-compose logs db Sadly, there's an error in logs and container has stopped. You only have to define a database password with environment variable POSTGRES_PASSWORD . Add this environment variable to docker-compose.yml.jsonnet template. local ddb = impor t 'ddb.docker.libjso nnet '; ddb.Compose( { services : { db : ddb.Image( \"postgres\" ) + { e n viro n me nt + : { POSTGRES_PASSWORD : \"ddb\" } } } } ) Jsonnet You may feel uncomfortable at first with Jsonnet , but this is a great tool and it brings a huge value to ddb. Here, we are merging the json object returned by ddb.Image(\"postgres\") with another object containing an environment key with environment variable values. + behind environment key name means that values from the new object are appended to values from the source one, instead of beeing replaced. To fully understand syntax and capabilities of jsonnet, you should take time to learn it . Run configure command again. ddb configure The generated docker-compose.yml file should now look like this: networks : {} services : db : environment : POSTGRES_PASSWORD : ddb image : postgres init : true restart : 'no' version : '3.7' volumes : {} docker-compose up -d docker-compose logs db The database should be up now ! Great :)","title":"Setup database"},{"location":"guides/psql-symfony-vue/#start-watch-mode","text":"As you now understand how ddb basics works, you may feel that running the ddb configure after each change is annoying. I'm pretty sure you want to stay a happy developer, so open a new interpreter, cd inside project directory, and run configure command with --watch flag. ddb --watch configure ddb is now running and listening for file events inside the project. Try to change database password inside the docker-compose.yml.jsonnet template file, it will immediately refresh docker-compose.yml . That's what we call Watch mode .","title":"Start watch mode"},{"location":"guides/psql-symfony-vue/#add-a-named-volume","text":"As you may already know, you need to setup a volume for data to be persisted if the container is destroyed. Let's stop and destroy all containers for now. docker-compose down Map a named volume to the db service inside docker-compose.yml.jsonnet . local ddb = impor t 'ddb.docker.libjso nnet '; ddb.Compose( { services : { db : ddb.Image( \"postgres\" ) { e n viro n me nt + : { POSTGRES_PASSWORD : \"ddb\" }, volumes+ : [ 'db - da ta : /var/lib/pos t gresql/da ta ' ] } } } ) Thanks to watch mode, changes are immediately generated in docker-compose.yml file networks : {} services : db : environment : POSTGRES_PASSWORD : ddb image : postgres init : true restart : 'no' volumes : - db-data:/var/lib/postgresql/data version : '3.7' volumes : db-data : {} db-data volume is now mapped on /var/lib/postgresql/data inside db service. And db-data volume has also been declared in the main volumes section ! Magic :) In fact, it's not so magic Those automated behavior provided by docker-compose.yml.jsonnet , like init and restart on each service, and global volumes declaration, are handled by ddb jsonnet library through ddb.Compose() function. For more information, check Jsonnet Feature section.","title":"Add a named volume"},{"location":"guides/psql-symfony-vue/#register-binary-from-image","text":"Database docker image contains binaries that you may need to run, like ... psql - PostgreSQL client binary. pg_dump - PostgreSQL command line tool to export data to a file. With docker-compose or raw docker, it may be really painful to find out the right docker-compose command to run those binary from your local environment. You may also have issues to read input data file, or to write output data file. With ddb, you can register those binaries right into docker-compose.yml.jsonnet to make them accessible from your local environment. local ddb = impor t 'ddb.docker.libjso nnet '; ddb.Compose( { services : { db : ddb.Image( \"postgres\" ) + ddb.Bi nar y( \"psql\" , \"/project\" , \"psql --dbname=postgresql://postgres:ddb@db/postgres\" ) + ddb.Bi nar y( \"pg_dump\" , \"/project\" , \"pg_dump --dbname=postgresql://postgres:ddb@db/postgres\" ) + { e n viro n me nt + : { POSTGRES_PASSWORD : \"ddb\" }, volumes+ : [ 'db - da ta : /var/lib/pos t gresql/da ta ' , ddb.pa t h.projec t + ' : /projec t ' ] } } } ) You should notice that some binary files have been generated in .bin directory : psql and pg_dump . Those binaries are available right now on your local environment. You can check their version. .bin/psql --version .bin/pg_dump --version But ... What the ... Where is all the docker hard stuff ? Of course, the docker hard stuff is still there. But it's hidden. ddb generates some shims for those binaries available in docker image, so you fell like those binaries are now installed on the project. But when invoking those binary shims, it creates a temporary container for the command's lifetime. Current working directory is mapped When registering binary from jsonnet this way, the project directory on your local environment should be mounted to /project inside the container. The working directory of the container is mapped to the working directory of your local environment. This allow ddb to match working directory from local environment and container, so you are able to access any files through a natural process. Default arguments --dbname=postgresql://postgres:ddb@db/postgres is added as a default argument to both command, so the commands won't require any connection settings. To bring psql and pg_dump shims into the path, you have to activate the project environment into your interpreter. $( ddb activate ) .bin directory is now in the interpreter's PATH , so psql and pg_dump are available anywhere. psql --version pg_dump --version Let's try to perform a dump with pg_dump . pg_dump -f dump.sql Great, the dump.sql file appears in your working directory ! Perfect. But if you check carefully your project directory, there's a problem here ! The dump file has been generated, but it is owned by root . You are not allowed to write or delete this file now. Thanks to sudo , you can do still delete it. sudo rm dump.sql But this suck ... As a developer, you are really disappointed ... And you are right. Nobody wants a file to be owned by root inside the project directory.","title":"Register binary from image"},{"location":"guides/psql-symfony-vue/#workaround-permission-issues","text":"To workaround those permission issues, ddb has automated the installation of fixuid inside a Dockerfile. Docker and permission issues Permission issues are a common pitfall while using docker on development environments. They are related to the way docker works and cannot really be fixed once for all. As you know, ddb like templates, so you are going to use Jinja for all Dockerfile files. By convention, custom Dockerfile.jinja lies in .docker/<image> directory, where <image> is to be replaced with effective image name. First step, create .docker/postgres/Dockerfile.jinja from postgres base image. FROM postgres USER postgres Second step, create .docker/postgres/fixuid.yml file. user : postgres group : postgres paths : - / - /var/lib/postgresql/data Fixuid Fixuid change uid and gid of the container user to match the host user, and it changes files ownerships as declared in fixuid.yml configuration file. Most of the time, user and group defined in the configuration file should match the user defined in Dockerfile, and paths should match the root directory and volume directories. When a fixuid.yml file is available next to a Dockerfile, ddb generates fixuid installation instructions into the Dockerfile , and entrypoint is changed to run fixuid before the default entrypoint. Last step, change in docker-compose.yml.jsonnet the service definition to use the newly created Dockerfile ( ddb.Image(\"postgres\") replaced to ddb.Build(\"postgres\") ), and set user to the host user uid/gid ( ddb.User() ). local ddb = impor t 'ddb.docker.libjso nnet '; ddb.Compose( { services : { db : ddb.Build( \"postgres\" ) + ddb.User() + ddb.Bi nar y( \"psql\" , \"/project\" , \"psql --dbname=postgresql://postgres:ddb@db/postgres\" ) + ddb.Bi nar y( \"pg_dump\" , \"/project\" , \"pg_dump --dbname=postgresql://postgres:ddb@db/postgres\" ) + { e n viro n me nt + : { POSTGRES_PASSWORD : \"ddb\" }, volumes+ : [ 'db - da ta : /var/lib/pos t gresql/da ta ' , ddb.pa t h.projec t + ' : /projec t ' ] } } } ) Stop containers, destroy data from existing database, and start again. docker-compose down -v docker-compose up -d Perform the dump. pg_dump -f dump.sql dump.sql is now owned by your own user, and as a developer, you are happy again :)","title":"Workaround permission issues"},{"location":"guides/psql-symfony-vue/#setup-php-apache-and-symfony-skeleton","text":"Then, we need to setup PHP FPM with it's related web server Apache. So, we are creating a new php service inside docker-compose.yml.jsonnet , based on a Dockerfile build. local ddb = impor t 'ddb.docker.libjso nnet '; ddb.Compose( { services : { ... php : ddb.Build( \"php\" ) + ddb.User() + { volumes+ : [ ddb.pa t h.projec t + \":/var/www/html\" , \"php-composer-cache:/composer/cache\" , \"php-composer-vendor:/composer/vendor\" ] } } ) And the related Dockerfile.jinja inside .docker/php directory. FROM php:7.3-fpm RUN docker-php-ext-install opcache RUN yes | pecl install xdebug && docker-php-ext-enable xdebug RUN apt-get update -y && \\ apt-get install -y libpq-dev && \\ rm -rf /var/lib/apt/lists/* && \\ docker-php-ext-install pdo pdo_pgsql ENV COMPOSER_HOME /composer ENV PATH /composer/vendor/bin: $PATH ENV COMPOSER_ALLOW_SUPERUSER 1 COPY --from = composer:2 /usr/bin/composer /usr/bin/composer RUN apt-get update -y && \\ apt-get install -y git zip unzip && \\ rm -rf /var/lib/apt/lists/* RUN mkdir -p \" $COMPOSER_HOME /cache\" \\ && mkdir -p \" $COMPOSER_HOME /vendor\" \\ && chown -R www-data:www-data $COMPOSER_HOME \\ && chown -R www-data:www-data /var/www VOLUME /composer/cache And fixuid.yml to fix file permission issues. user : www-data group : www-data paths : - / - /composer/cache Then build the docker image with docker-compose build . Composer has been installed in the image, so let's make it available by registering a binary into docker-compose.yml.jsonnet . We can also register the php binary for it to be available locally too. local ddb = import 'ddb.docker.libjsonnet'; ddb.Compose({ services: { ... php: ddb.Build(\"php\") + ddb.User() + ddb.Binary(\"composer\", \"/var/www/html\", \"composer\") + ddb.Binary(\"php\", \"/var/www/html\", \"php\") + { volumes+: [ ddb.path.project + \":/var/www/html\", \"php-composer-cache:/composer/cache\", \"php-composer-vendor:/composer/vendor\" ] } }, }) And activate the project, with $(ddb activate) . The composer command in now available right in your PATH. $ composer --version Composer version 1 .10.10 2020 -08-03 11 :35:19 $ php --version PHP 7 .3.10 ( cli ) ( built: Oct 17 2019 15 :09:28 ) ( NTS ) Copyright ( c ) 1997 -2018 The PHP Group Zend Engine v3.3.10, Copyright ( c ) 1998 -2018 Zend Technologies with Xdebug v2.9.6, Copyright ( c ) 2002 -2020, by Derick Rethans Now PHP and composer are available, you can generate the symfony skeleton inside a backend directory. composer create-project symfony/skeleton backend We also need a web service for Apache configured with PHP. Here's the docker-compose.yml.jsonnet part. local ddb = impor t 'ddb.docker.libjso nnet '; local domai n _ex t = s t d.ex t Var( \"core.domain.ext\" ); local domai n _sub = s t d.ex t Var( \"core.domain.sub\" ); local domai n = s t d.joi n ('.' , [ domai n _sub , domai n _ex t ] ); ddb.Compose( { services : { ... web : ddb.Build( \"web\" ) + ddb.Vir tual Hos t ( \"80\" , domai n ) { volumes+ : [ ddb.pa t h.projec t + \":/var/www/html\" , ddb.pa t h.projec t + \"/.docker/web/apache.conf:/usr/local/apache2/conf/custom/apache.conf\" , ] }, } ) Use std.extVar(...) inside jsonnet to read a configuration property As you can see here, we are using jsonnet features to build the domain name and setup the traefik configuration for the virtualhost. Configuration properties are available inside all template engines and can be listed with ddb config --variables As with the php service, a .docker/web/Dockerfile.jinja is created to define the image build. FROM httpd:2.4 RUN mkdir -p /usr/local/apache2/conf/custom \\ && mkdir -p /var/www/html \\ && sed -i '/LoadModule proxy_module/s/^#//g' /usr/local/apache2/conf/httpd.conf \\ && sed -i '/LoadModule proxy_fcgi_module/s/^#//g' /usr/local/apache2/conf/httpd.conf \\ && echo >> /usr/local/apache2/conf/httpd.conf && echo 'Include conf/custom/*.conf' >> /usr/local/apache2/conf/httpd.conf RUN sed -i '/LoadModule headers_module/s/^#//g' /usr/local/apache2/conf/httpd.conf RUN sed -i '/LoadModule rewrite_module/s/^#//g' /usr/local/apache2/conf/httpd.conf apache.conf specified in docker compose volume mount is also generated from a jinja file, apache.conf.jinja . It is used to inject domain name and docker compose network name, for the domain to be centralized into ddb.yml configuration and ease various environment deployements (stage, prod). <VirtualHost *:80> ServerAdmin webmaster@{{core.domain.sub}}.{{core.domain.ext}} ServerName api.{{core.domain.sub}}.{{core.domain.ext}} DocumentRoot /var/www/html/backend/public <Directory \"/var/www/html/backend/public/\"> DirectoryIndex index.php AllowOverride All Order allow,deny Allow from all Require all granted # symfony configuration from https://github.com/symfony/recipes-contrib/blob/master/symfony/apache-pack/1.0/public/.htaccess # By default, Apache does not evaluate symbolic links if you did not enable this # feature in your server configuration. Uncomment the following line if you # install assets as symlinks or if you experience problems related to symlinks # when compiling LESS/Sass/CoffeScript assets. # Options FollowSymlinks # Disabling MultiViews prevents unwanted negotiation, e.g. \"/index\" should not resolve # to the front controller \"/index.php\" but be rewritten to \"/index.php/index\". <IfModule mod_negotiation.c> Options -MultiViews </IfModule> <IfModule mod_rewrite.c> RewriteEngine On # Determine the RewriteBase automatically and set it as environment variable. # If you are using Apache aliases to do mass virtual hosting or installed the # project in a subdirectory, the base path will be prepended to allow proper # resolution of the index.php file and to redirect to the correct URI. It will # work in environments without path prefix as well, providing a safe, one-size # fits all solution. But as you do not need it in this case, you can comment # the following 2 lines to eliminate the overhead. RewriteCond %{REQUEST_URI}::$1 ^(/.+)/(.*)::\\2$ RewriteRule ^(.*) - [E=BASE:%1] # Sets the HTTP_AUTHORIZATION header removed by Apache RewriteCond %{HTTP:Authorization} . RewriteRule ^ - [E=HTTP_AUTHORIZATION:%{HTTP:Authorization}] # Redirect to URI without front controller to prevent duplicate content # (with and without `/index.php`). Only do this redirect on the initial # rewrite by Apache and not on subsequent cycles. Otherwise we would get an # endless redirect loop (request -> rewrite to front controller -> # redirect -> request -> ...). # So in case you get a \"too many redirects\" error or you always get redirected # to the start page because your Apache does not expose the REDIRECT_STATUS # environment variable, you have 2 choices: # - disable this feature by commenting the following 2 lines or # - use Apache >= 2.3.9 and replace all L flags by END flags and remove the # following RewriteCond (best solution) RewriteCond %{ENV:REDIRECT_STATUS} ^$ RewriteRule ^index\\.php(?:/(.*)|$) %{ENV:BASE}/$1 [R=301,L] # If the requested filename exists, simply serve it. # We only want to let Apache serve files and not directories. RewriteCond %{REQUEST_FILENAME} -f RewriteRule ^ - [L] # Rewrite all other queries to the front controller. RewriteRule ^ %{ENV:BASE}/index.php [L] </IfModule> <IfModule !mod_rewrite.c> <IfModule mod_alias.c> # When mod_rewrite is not available, we instruct a temporary redirect of # the start page to the front controller explicitly so that the website # and the generated links can still be used. RedirectMatch 307 ^/$ /index.php/ # RedirectTemp cannot be used instead </IfModule> </IfModule> </Directory> SetEnvIf Authorization \"(.*)\" HTTP_AUTHORIZATION=$1 <FilesMatch \\.php$> SetHandler \"proxy:fcgi://php.{{docker.compose.network_name}}:9000\" </FilesMatch> </VirtualHost> And now, we are ready start all containers : docker-compose up -d . Run ddb info command to check the URL of your virtualhost for the web service. You should be able to view Symfony landing page at http://ddb-quickstart.test and https://ddb-quickstart.test . You may have to restart traefik container If you have some issues with certificate validity on the https:// url, you may need to restart traefik container : docker restart traefik .","title":"Setup PHP, Apache and Symfony Skeleton"},{"location":"guides/psql-symfony-vue/#setup-vuejs-and-vue-cli","text":"To be continued","title":"Setup VueJS and Vue CLI"}]}